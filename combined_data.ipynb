{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reel Returns**\n",
    "#### *Machine Learning Insights into Movie Profitability*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis;\n",
    "\n",
    "It is accepted as general knowledge that when economic times are harder, the entertainment industry tends to do well. Assuming this is true, there should be a strong correlation between any given movie's critical and financial successes and indicators of the economic climate at that movie's release date. By examining both movie data and economic indicators from 1981 to 2023 these relationships will be shown through features such as a given movie's ratings, return on investment, and economic features such as consumer spending habits and employment rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV #Retained for KNN commented cell\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing gdown (comment out if uneeded)\n",
    "%pip install gdown --quiet\n",
    "\n",
    "# Importing gdown\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerous factors contribue to the successes and failures within the film industry, both on critical and financial scales. Using a simplified view to focus on more generalized classifications, as well as to meet the puposes of our modelling, several contributing factors were chosen to highlight and predict each scale of success within our selected dataset. The features ultimately chosen were;\n",
    "\n",
    "* Vote Average (a given movie's rating from zero (0) to ten (10))\n",
    "* Vote Count\n",
    "* Revenue (total earnings in USD for a given movie)\n",
    "* Runtime\n",
    "* Budget (total expenses in USD to produce and promote a given movie)\n",
    "* Title\n",
    "* Original Title\n",
    "* Genres\n",
    "* Production Companies\n",
    "\n",
    "To create our `critical_success` indicator - a classification as to how well recieved a title was by fans and critics - we focused on the `vote_average` feature to create guidelines for ranges of scores.\n",
    "\n",
    "To engineer a `financial_success` indicator - a measure of what level of returns a tital produced - we used the percentage calculated as `budget` subtracted from `revenue`, then divided by `budget`, and compared the results to industry standard breakpoints.\n",
    "\n",
    "---\n",
    "\n",
    "The following dataset is courtesy of __[Kaggle](https://www.kaggle.com/)__.\n",
    "\n",
    "**__[TMDB_all_movies.csv](https://www.kaggle.com/datasets/alanvourch/tmdb-movies-daily-updates?select=TMDB_all_movies.csv)__**\n",
    "\n",
    "Per the dataset description;\n",
    "\n",
    "* This dataset was curated from __[The Movie Database](https://www.themoviedb.org/?language=en-US)__, and inspired by __[asaniczka](https://www.kaggle.com/asaniczka)__'s __[dataset](https://www.kaggle.com/datasets/asaniczka/tmdb-movies-dataset-2023-930k-movies)__\n",
    "* While updated daily, the dataset used in this notebook was downloaded on **7/7/2024**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring `url` and `output` for dataset\n",
    "url = 'https://drive.google.com/file/d/1Om73B4In4cHj0Rf6aIGOi3-8iXWbDAWg/view?usp=sharing'\n",
    "output = 'Resources/TMDB_all_movies.csv'\n",
    "\n",
    "# Downloading dataset\n",
    "gdown.download(url, output, fuzzy=True, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in dataset\n",
    "tmdb_data = pd.read_csv(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions\n",
    "\n",
    "The following function will be used code to help streamline the flow of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Universal functions\n",
    "\n",
    "Applicable to all datasets\n",
    "\n",
    "**Null Percentages**\n",
    "\n",
    "Calculating the percentage of null values, by feature, in a given dataset\n",
    "\n",
    "**Records Total**\n",
    "\n",
    "Confirming the total records for a given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to calculate the percentage of null values in\n",
    "# each feature of given DF\n",
    "def null_percentages(df):\n",
    "    return df.isnull().sum()/len(df)*100\n",
    "\n",
    "# Defining a function to display the total records for a given DF\n",
    "def records_total(df):\n",
    "    print(f'Total records: {df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Situational functions\n",
    "\n",
    "Applicable to some datasets or situations\n",
    "\n",
    "**Records Total by Feature Value**\n",
    "\n",
    "Confirming the total records for a given dataset based on a stated value for a single selected feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to display the total records for a given DF based on\n",
    "# a stated value for a single selected feature\n",
    "def records_total_feat(df, feature, value):\n",
    "    print(f\"Total selected records for '{feature}' of '{value}': {df.loc[df[feature] == value].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing `tmdb_data`\n",
    "tmdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying `null_percentages` to `tmdb_data`\n",
    "null_percentages(tmdb_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing features and dataset\n",
    "\n",
    "Dropping uneeded features and reducing dataset to domestic released movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of uneeded features\n",
    "movie_cols_to_drop = [\n",
    "    'imdb_id','overview', 'tagline', 'director_of_photography', 'music_composer'\n",
    "]\n",
    "\n",
    "# Dropping uneeded features\n",
    "tmdb_data.drop(columns=movie_cols_to_drop,inplace=True)\n",
    "\n",
    "# Reapplying `null_percentages` to `tmdb_data`\n",
    "null_percentages(tmdb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming values of `status`\n",
    "tmdb_data['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming values of `production_countries`\n",
    "tmdb_data['production_countries'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing dataset to only `Released` movies produced in `United States of America`\n",
    "df_movies = tmdb_data.loc[\n",
    "    (tmdb_data['production_countries'] == 'United States of America') &\n",
    "    (tmdb_data['status'] == 'Released')\n",
    "].copy()\n",
    "\n",
    "# Applying `null_percentages` to `us_tmdb_df`\n",
    "null_percentages(df_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering\n",
    "\n",
    "Engineering the two success target values, converting `rlease_date` to datetime, and reducing the `genres` feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critical success\n",
    "\n",
    "With a rating scale of zero (0) to ten (10), ranges can be established to break a given movie's critical success down by the following scale;\n",
    "\n",
    "* **0 to 2.5**: `panned`\n",
    "* **2.5 to 5**: `alright`\n",
    "* **5 to 7.5**: `well liked`\n",
    "* **7.5 to 10**: `critical success`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming values of `vote_average`\n",
    "df_movies['vote_average'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating bins to rate values of `vote_average`\n",
    "bins = [0, 2.5, 5, 7.5, 10]\n",
    "\n",
    "# Labelling bins\n",
    "critical_success = ['panned', 'alright', 'well liked', 'critical success']\n",
    "\n",
    "# Slicing the data and placing values in `critical_success`\n",
    "df_movies['critical_success'] = pd.cut(\n",
    "    df_movies['vote_average'], bins, labels=critical_success, include_lowest=True\n",
    ")\n",
    "\n",
    "# Confirming binned correctly\n",
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Financial success\n",
    "\n",
    "Classifying a given movie's financial success can be accomplished by generating a percentage to represent the return on investment (`roi`) calculated as;\n",
    "\n",
    "> ((`revenue`-`budget`)/`budget`) * 100\n",
    "\n",
    "The resulting value can then be compared to industry standard breakpoints to describe the folling classifications; \n",
    "\n",
    "* **Less than 0%**: `failure`\n",
    "* **Exactly 0%**: `broke even`\n",
    "* **Between 0% and 50%**: `modest returns`\n",
    "* **Between 50% and 100%**: `moderate returns`\n",
    "* **Between 100% and 500%**: `excellent returns`\n",
    "* **Over 500%**: `extraordinary returns`\n",
    "\n",
    "*Note: For the purposes of our modeling, only records with a `budget` NOT equal to zero (0) will be retained*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming total records with a `budget` of `0\n",
    "records_total_feat(df_movies, 'budget', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only records with a `budget` NOT equal to `0`\n",
    "df_movies = df_movies[df_movies['budget'] != 0].copy()\n",
    "\n",
    "# Confirming new total records with a `budget` of `0`\n",
    "records_total_feat(df_movies, 'budget', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating `roi` as described above\n",
    "df_movies['roi'] = (\n",
    "    (df_movies['revenue'] - df_movies['budget'])/df_movies['budget']\n",
    ") * 100\n",
    "\n",
    "# Confirming values of `roi`\n",
    "df_movies['roi'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming total records with a `roi` of `0`\n",
    "records_total_feat(df_movies, 'roi', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating bins to rate values of `roi`\n",
    "bins = [-float('inf'), 0, 50, 100, 500, float('inf')]\n",
    "\n",
    "# Labelling bins\n",
    "financial_success = [\n",
    "    'failure', 'modest returns', 'moderate returns',\n",
    "    'excellent returns', 'extraordinary returns'\n",
    "]\n",
    "\n",
    "# Slicing the data and placing values in `financial_success`\n",
    "df_movies['financial_success'] = pd.cut(\n",
    "    df_movies['roi'], bins, labels=financial_success, include_lowest=True\n",
    ")\n",
    "\n",
    "# Adding classification 'broke even' to `financial_success`\n",
    "df_movies['financial_success'] = df_movies['financial_success'].cat.add_categories('broke even')\n",
    "\n",
    "# Classifying where `roi` equals `0` as `broke even`\n",
    "df_movies.loc[df_movies['roi'] == 0, 'financial_success'] = 'broke even'\n",
    "\n",
    "# Confirming when `roi` is `0`, `financial_success` is 'broke even'\n",
    "df_movies.loc[df_movies['roi'] == 0, 'financial_success'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datetime values\n",
    "\n",
    "Converting `release_date` to a datetime value and extracting the year and month for later concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming dtype for `release_date`\n",
    "df_movies['release_date'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'release_date' to datetime\n",
    "df_movies['release_date'] = pd.to_datetime(\n",
    "    df_movies['release_date'], format='%Y-%m-%d', errors='coerce'\n",
    ")\n",
    "\n",
    "# Confirming conversion\n",
    "df_movies['release_date'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming records with `NaT` (Not a Time) values for `release_date`\n",
    "print(\n",
    "    'Total records where `release_date` has a `NaT` value: ' +\\\n",
    "    str(df_movies['release_date'].isna().sum())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping records where 'release_date' is `NaT`\n",
    "df_movies.dropna(subset=['release_date'], inplace=True)\n",
    "\n",
    "# Reconfirming records with `NaT` (Not a Time) values for `release_date` \n",
    "print(\n",
    "    'Total records where `release_date` has a `NaT` value: ' +\\\n",
    "    str(df_movies['release_date'].isna().sum())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting year, month, and day from 'release_date'\n",
    "df_movies['released_year'] = df_movies['release_date'].dt.year\n",
    "df_movies['released_month'] = df_movies['release_date'].dt.month\n",
    "df_movies['released_day'] = df_movies['release_date'].dt.day\n",
    "\n",
    "# Converting `released_year`, `released_month`, and `released_day` to integers\n",
    "df_movies['released_year'] = df_movies['released_year'].astype(int)\n",
    "df_movies['released_month'] = df_movies['released_month'].astype(int)\n",
    "df_movies['released_day'] = df_movies['released_day'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Economics Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While classifying economic states is a complex and nuanced issue, it is not unreasonable to draw more broad-strokes generalizations about a given timeframe based on more limited factors. To serve the purposes of our modeling, the following three factors were chosen to highlight the economic status at a given movie's release date;\n",
    "\n",
    "* Consumer Confidence Indicator (CCI)\n",
    "* Consumer Price Index (CPI)\n",
    "* Unemployment Rate\n",
    "\n",
    "These features stand as adequate datapoints to answer three respective questions;\n",
    "\n",
    "* How likely are people to be spending money?\n",
    "* How much do things cost when they do spend money?\n",
    "* How many people have jobs to earn money to spend?\n",
    "\n",
    "As detailed below, this information came as monthly measures over several decades. To create our `Economic Climate` indicator - a classification as to whether or not the economics of a given time were on the better side for consumers - we will need to calculate a rolling 12-month percent change in the mean of those monthly values in order to show if a given feature was on a positive or negative trend for the provided period.\n",
    "\n",
    "---\n",
    "\n",
    "The following datasets are courtesy of __[Kaggle](https://www.kaggle.com/)__.\n",
    "\n",
    "**__['CCI_OECD.csv'](https://www.kaggle.com/datasets/iqbalsyahakbar/cci-oecd)__**\n",
    "\n",
    "*renamed from `DP_LIVE_16112023095843236.csv`*\n",
    "\n",
    "Per the Organisation for Economic Co-operation and Development (OECD);\n",
    "\n",
    "* The CCI is an indication of developments for future households' consumption and saving based on expected financial situation, sentiment regarding the general economic situation, employment status, and capacity for savings\n",
    "* An indicator above `100` indicates an optimistic outlook and a greater likliehood to spend money over cautious saving\n",
    "* An indicator below `100` indicates a pessimistic outlook and both a higher likeliehood to save money and a lower tendency to consume\n",
    "\n",
    "**__['US_inflation_rates.csv'](https://www.kaggle.com/datasets/pavankrishnanarne/us-inflation-dataset-1947-present)__**\n",
    "\n",
    "Per the dataset description;\n",
    "\n",
    "* The CPI is a critical economic indicator for measuring the purchasing power of money over time, measuring the average change over time in the prices paid by urban consumers for goods and services\n",
    "* The CPI is the value at the end of the respective month\n",
    "\n",
    "---\n",
    "\n",
    "The following dataset is courtesy of the __[Economic Policy Instituteâ€™s (EPI) State of Working America Data Library](https://www.epi.org/data/)__.\n",
    "\n",
    "**__['Unemployment.csv'](https://www.epi.org/data/#?subject=unemp)__**\n",
    "\n",
    "Per EPI description;\n",
    "\n",
    "* Unemployment is the share of the labor force wihout a job\n",
    "* Monthly percentages calculated as a rolling 12-month average (mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in datasets\n",
    "df_unemp = pd.read_csv(\"./Resources/EPI Data Library - Unemployment.csv\")\n",
    "df_cci = pd.read_csv(\"./Resources/CCI_OECD.csv\")\n",
    "df_inflation = pd.read_csv(\"./Resources/US_inflation_rates.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions\n",
    "\n",
    "Since each dataset will need similar preprocessing, the following functions will be used to help streamline the flow of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Universal functions\n",
    "\n",
    "Applicable to all datasets\n",
    "\n",
    "**EDA routine**\n",
    "\n",
    "Labelling and displaying pertinant information about a given dataset for the purposes of expedited EDA\n",
    "\n",
    "**Copying datasets**\n",
    "\n",
    "Creating a working copy of a given dataset to preserve the original DF with unneeded features dropped\n",
    "\n",
    "**Renaming needed features**\n",
    "\n",
    "Renaming selected features for a given dataset\n",
    "\n",
    "**Rolling mean and mean percent change**\n",
    "\n",
    "Calculating the rolling 12-month mean and the rolling 12-month percent change for a given feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to display the `.describe()`, `.shape`, and `.dtypes`\n",
    "# for a given DF\n",
    "def eda_routine(df):\n",
    "    print('Describe:')\n",
    "    display(df.describe())\n",
    "    print(f'Shape: {df.shape}\\n')\n",
    "    print(f'Data types:')\n",
    "    display(df.dtypes)\n",
    "\n",
    "# Defining a function to copy a dataset with only the needed features\n",
    "def copy_df(df, features_to_keep):\n",
    "    df_copy = df[features_to_keep].copy()\n",
    "    return df_copy\n",
    "\n",
    "# Defining a function to rename needed features\n",
    "def rename_features(df, feature1, feature1new, feature2, feature2new):\n",
    "    df.rename(columns={\n",
    "        feature1: feature1new,\n",
    "        feature2: feature2new\n",
    "    }, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Defining a function to calculate the rolling 12-month means and percent changes\n",
    "# for a given feature\n",
    "def rolling_calcs(df, feature, feature_mean, feature_pct_chng):\n",
    "    df[feature_mean] = df[feature].rolling(window=12).mean()\n",
    "    df[feature_pct_chng] = df[feature_mean].pct_change(periods=12) * 100\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Situational functions\n",
    "\n",
    "Applicable to select datasets\n",
    "\n",
    "**Datetime indexing**\n",
    "\n",
    "Converting the feature containing the raw datetime information into a suitable datetime index\n",
    "\n",
    "*Cannot be used on `Unemployment` dataset*\n",
    "\n",
    "**Removing '%'**\n",
    "\n",
    "Removing the `'%'` from a given feature and converting the remaining `object` dtype to `float`\n",
    "\n",
    "*Specifically for `Unemployment` dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to set a `Date` feature as a datetime index\n",
    "def datetime_index(df, datetime_feature):\n",
    "    df[datetime_feature] = pd.to_datetime(df[datetime_feature])\n",
    "    df.set_index(datetime_feature, inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "# Defining a function to remove '%' and convert data `float`\n",
    "def convert_percentage(feature):\n",
    "    return float(feature.strip('%'))\n",
    "\n",
    "# Defining a function to apply `convert_percentage`\n",
    "def apply_percentage(df, feature):\n",
    "    df[feature] = df[feature].apply(convert_percentage)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CCI\n",
    "\n",
    "This dataset came with internaitonal records and uneeded features, so only records for US CCI will be retained. Once those records have been selected, the resulting DF will need to be prepared for concatenation with the remainined economic datasets. To do this, the `TIME` feature will be converted to datetime and set as the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing `df_cci`\n",
    "df_cci.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying `eda_routine` to `df_cci`\n",
    "eda_routine(df_cci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing dataset\n",
    "\n",
    "Reducing the dataset to only domestic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming values of `LOCATION`\n",
    "df_cci['LOCATION'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying domestic data from `df_cci` to `df_cci_us`\n",
    "df_cci_us = df_cci.loc[df_cci['LOCATION'] == 'USA'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying `df_cci_us` and dropping uneeded features\n",
    "df_cci_form = copy_df(df_cci_us, ['TIME', 'Value'])\n",
    "\n",
    "# Renamining retained features\n",
    "df_cci_form = rename_features(\n",
    "    df_cci_form, 'TIME', 'Date', 'Value', 'CCI Value'\n",
    ")\n",
    "\n",
    "# Converting `Date` to a datetime index\n",
    "df_cci_form = datetime_index(df_cci_form, 'Date')\n",
    "\n",
    "# Calculating rolling 12-month means and percent change in means\n",
    "df_cci_form = rolling_calcs(\n",
    "    df_cci_form, 'CCI Value', 'CCI Rolling Mean', 'CCI Rolling Percent Change'\n",
    ")\n",
    "\n",
    "# Confirming `df_cci_form` ready to concatenate\n",
    "display(df_cci_form.head())\n",
    "display(df_cci_form.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inflation\n",
    "\n",
    "Seeing as the dataset came with only the needed features, little will be needed to prepare the DF for concatenation with the other economic datasets. `date` will be converted to datetime and set as the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing `df_inflation`\n",
    "df_inflation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying `eda_routine` to `df_inflation`\n",
    "eda_routine(df_inflation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying `df_inflation` and dropping uneeded features\n",
    "df_inflation_form = copy_df(df_inflation, ['date', 'value'])\n",
    "\n",
    "# Renamining retained features\n",
    "df_inflation_form = rename_features(\n",
    "    df_inflation_form, 'date', 'Date', 'value', 'CPI Value'\n",
    ")\n",
    "\n",
    "# Converting `Date` to a datetime index\n",
    "df_inflation_form = datetime_index(df_inflation_form, 'Date')\n",
    "\n",
    "# Calculating rolling 12-month means and percent change in means\n",
    "df_inflation_form = rolling_calcs(\n",
    "    df_inflation_form,\n",
    "    'CPI Value',\n",
    "    'CPI Rolling Mean',\n",
    "    'CPI Rolling Percent Change'\n",
    ")\n",
    "\n",
    "# Confirming `df_inflation_form` ready to concatenate\n",
    "display(df_inflation_form.head())\n",
    "display(df_inflation_form.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unemployment\n",
    "\n",
    "This dataset came with uneeded features that will need to be dropped, as well as the needed features will need to be converted to `float`. Additionally, the `Date` feature will need to be converted to datetime and set to the index in preparation for concatenation with the other economic datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing `df_unemp`\n",
    "df_unemp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying `eda_routine` to `df_unemp`\n",
    "eda_routine(df_unemp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying defined functions (first pass)\n",
    "\n",
    "Given the nature of the `Date` feature in this dataset, the datetime indexing will need to be handled outside of the defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying `df_unemp` and dropping unneeded features\n",
    "df_unemp_form = copy_df(df_unemp, ['Date', 'All'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming and indexing\n",
    "\n",
    "This dataset only needed one feature, `All`, to be renmaned, therefore the `rename_features` defined function is not applicable\n",
    "\n",
    "Additionally, the `Date` feature will need to be engineered into a workable datetime feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the reatined feature\n",
    "df_unemp_form.rename(columns={'All': 'Unemployment Rate (%)'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary of Months\n",
    "month_map = {\n",
    "    'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
    "    'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
    "}\n",
    "\n",
    "# Mapping integer month values to `Date Month`\n",
    "df_unemp_form['Date Month'] = df_unemp_form['Date'].str.slice(0,3).map(month_map)\n",
    "\n",
    "# Slicing `Date Year`\n",
    "df_unemp_form['Date Year'] = df_unemp_form['Date'].str.slice(4,8)\n",
    "\n",
    "# Converting `Date` to datetime using `Date Month` and `Date Year`\n",
    "df_unemp_form['Date'] = pd.to_datetime({\n",
    "    'year': df_unemp_form['Date Year'],\n",
    "    'month': df_unemp_form['Date Month'],\n",
    "    'day': 1\n",
    "})\n",
    "\n",
    "# Dropping engineered features `Date Month` and `Date Year`\n",
    "df_unemp_form.drop(columns=['Date Month', 'Date Year'], inplace=True)\n",
    "\n",
    "# Setting `Date` as index\n",
    "df_unemp_form.set_index('Date', inplace=True)\n",
    "\n",
    "# Ensuring index is sorted with ascending dates\n",
    "df_unemp_form.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying defined functions (second pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying `apply_percentage` to `Unemployment Rate (%)`\n",
    "df_unemp_form = apply_percentage(df_unemp_form, 'Unemployment Rate (%)')\n",
    "\n",
    "# Calculating rolling 12-month means and percent change in means\n",
    "df_unemp_form = rolling_calcs(\n",
    "    df_unemp_form,\n",
    "    'Unemployment Rate (%)',\n",
    "    'Unemployment Rate (%) Rolling Mean',\n",
    "    'Unemployment Rate Rolling Percent Change',\n",
    ")\n",
    "\n",
    "# Confirming `df_unemp_form` ready to concatenate\n",
    "display(df_unemp_form.head())\n",
    "display(df_unemp_form.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Economics\n",
    "\n",
    "With all datasets set to a monthly datetime index, the relevent features can be combined into one DF, and any NaN records can be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconfirming total records and features for datasets\n",
    "print('CCI:')\n",
    "records_total(df_cci_form)\n",
    "print('\\nInflation (CPI):')\n",
    "records_total(df_inflation_form)\n",
    "print('\\nUnemployment:')\n",
    "records_total(df_unemp_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the economic datasets into `df_economics`\n",
    "df_economics = pd.concat(\n",
    "    [\n",
    "        df_cci_form,\n",
    "        df_inflation_form,\n",
    "        df_unemp_form\n",
    "    ], axis=1, join='outer'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `NaN` records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming total records and features\n",
    "df_economics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking total `NaN` records\n",
    "df_economics.isna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping `NaN` records\n",
    "df_economics.dropna(inplace=True)\n",
    "\n",
    "# Confirming remaining records\n",
    "records_total(df_economics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming final economic DF\n",
    "display(df_economics.head())\n",
    "display(df_economics.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering\n",
    "\n",
    "Engineering the economic target values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Economic Climate\n",
    "\n",
    "As stated, the goal is to create an indicator for `Economic Climate` based on broad-strokes observations of our datasets. Having calculated the rolling 12-month percent change for each feature - based off the rolling 12-month mean - we can look for a positive or negative change in values and flag the movement accordingly. From there, we can make the following simple statements;\n",
    "\n",
    "* For **CCI**, a positive change is \"good\", as it indicates an increase in the likelihood of consumers to spend money\n",
    "* For **CPI**, a negative change is \"good\", as it indicates a decrease in the costs for goods and services\n",
    "* For **Unemployment Rate**, a negative change is \"good\", as it indicates an incrase in the population active in the workforce\n",
    "\n",
    "Therefore, we can interpret movement contrary to those changes as \"bad\". With this simplified view of the features, we can draw a classification as follows;\n",
    "\n",
    "* If **at least two (2) features** have a \"good\" value, we can set `Economic Climate` to `Comfortable to Good`\n",
    "* If **at least two (2) features** have a \"bad\" value, we can set `Economic Climate` to `Lean to Bad`\n",
    "\n",
    "In this way, we can gague whether the ecnomic state at a given rlease date supports or disproves our hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming ranges and statistics of `df_economics`\n",
    "df_economics.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of features\n",
    "features_to_flag = [\n",
    "    'CCI Rolling Percent Change',\n",
    "    'CPI Rolling Percent Change',\n",
    "    'Unemployment Rate Rolling Percent Change'\n",
    "]\n",
    "\n",
    "# Looping through `features_to_flag` to assign `positive` and `negative` indicators\n",
    "for col in df_economics[features_to_flag].columns:\n",
    "    new_col = str(col) + ' Flag'\n",
    "    df_economics.loc[df_economics[col] > 0, new_col] = 'positive'\n",
    "    df_economics.loc[df_economics[col] <= 0, new_col] = 'negative'\n",
    "\n",
    "# Creating a of list flagged features\n",
    "flag_cols = [\n",
    "    'CCI Rolling Percent Change Flag',\n",
    "    'CPI Rolling Percent Change Flag',\n",
    "    'Unemployment Rate Rolling Percent Change Flag'\n",
    "]\n",
    "\n",
    "# Confirming indicators applied\n",
    "df_economics[flag_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of conditions and classifications\n",
    "conditions = [\n",
    "    ((df_economics[flag_cols[0]] == 'positive') &   # CCI = 'positive'/good\n",
    "    (df_economics[flag_cols[1]] == 'positive') &    # CPI = 'positive'/bad\n",
    "    (df_economics[flag_cols[2]] == 'positive'),     # Unemplyment = 'positive'/bad\n",
    "    'Lean to Bad'),\n",
    "    ((df_economics[flag_cols[0]] == 'positive') &   # CCI = 'positive'/good\n",
    "    (df_economics[flag_cols[1]] == 'positive') &    # CPI = 'positive'/bad\n",
    "    (df_economics[flag_cols[2]] == 'negative'),     # Unemplyment = 'negative'/good\n",
    "    'Comfortable to Good'),\n",
    "    ((df_economics[flag_cols[0]] == 'positive') &   # CCI = 'positive'/good\n",
    "    (df_economics[flag_cols[1]] == 'negative') &    # CPI = 'negative'/good\n",
    "    (df_economics[flag_cols[2]] == 'positive'),     # Unemplyment = 'positive'/bad\n",
    "    'Comfortable to Good'),\n",
    "    ((df_economics[flag_cols[0]] == 'negative') &   # CCI = 'negative'/bad\n",
    "    (df_economics[flag_cols[1]] == 'positive') &    # CPI = 'positive'/bad\n",
    "    (df_economics[flag_cols[2]] == 'positive'),     # Unemplyment = 'positive'/bad\n",
    "    'Lean to Bad'),\n",
    "    ((df_economics[flag_cols[0]] == 'negative') &   # CCI = 'negative'/bad\n",
    "    (df_economics[flag_cols[1]] == 'negative') &    # CPI = 'negative'/good\n",
    "    (df_economics[flag_cols[2]] == 'positive'),     # Unemplyment = 'positive'/bad\n",
    "    'Lean to Bad'),\n",
    "    ((df_economics[flag_cols[0]] == 'negative') &   # CCI = 'negative'/bad\n",
    "    (df_economics[flag_cols[1]] == 'positive') &    # CPI = 'positive'/bad\n",
    "    (df_economics[flag_cols[2]] == 'negative'),     # Unemplyment = 'negative'/good\n",
    "    'Lean to Bad'),\n",
    "    ((df_economics[flag_cols[0]] == 'positive') &   # CCI = 'positive'/good\n",
    "    (df_economics[flag_cols[1]] == 'negative') &    # CPI = 'negative'/good\n",
    "    (df_economics[flag_cols[2]] == 'negative'),     # Unemplyment = 'negative'/good\n",
    "    'Comfortable to Good'),\n",
    "    ((df_economics[flag_cols[0]] == 'negative') &   # CCI = 'negative'/bad\n",
    "    (df_economics[flag_cols[1]] == 'negative') &    # CPI = 'negative'/good\n",
    "    (df_economics[flag_cols[2]] == 'negative'),     # Unemplyment = 'negative'/good\n",
    "    'Comfortable to Good')\n",
    "]\n",
    "\n",
    "# Declaring `Economic Climate` with a `PLACEHOLDER` value\n",
    "df_economics['Economic Climate'] = 'PLACEHOLDER'\n",
    "\n",
    "# Applying conditions and classifications to `Economic Climate`\n",
    "for condition, classification in conditions:\n",
    "    df_economics.loc[condition, 'Economic Climate'] = classification\n",
    "\n",
    "# Confirming classifications applied\n",
    "df_economics['Economic Climate'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations\n",
    "\n",
    "Generating visualizations for `df_economics`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features and baselines\n",
    "\n",
    "Declaring some helpful lists and values for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of features\n",
    "features_to_plot = [\n",
    "    'CCI Value',\n",
    "    'CPI Value',\n",
    "    'Unemployment Rate (%)'\n",
    "]\n",
    "\n",
    "# Creating a value of `0` to show positive and negative values\n",
    "zero_line = pd.DataFrame({\n",
    "    'Date': df_economics.index,\n",
    "    'val': [x for x in 0*df_economics[features_to_flag[2]]]\n",
    "})\n",
    "zero_line.set_index('Date', inplace=True)\n",
    "\n",
    "# Creating a value of `100` to show break point for CCI\n",
    "hundred_line = pd.DataFrame({\n",
    "    'Date': df_economics.index,\n",
    "    'val': [x for x in (0*df_economics[features_to_flag[0]])+100]\n",
    "})\n",
    "hundred_line.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing trends for `CCI Value`\n",
    "plt.plot(hundred_line, color='black', linestyle='--')\n",
    "plt.plot(df_economics[features_to_plot[0]], label='CCI', color='blue')\n",
    "plt.title('Values above 100 (visualized)\\n indicate consumers more likely to spend vs save')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing trends for `CCI Rolling Percent Change`\n",
    "plt.plot(zero_line, color='black', linestyle='--')\n",
    "plt.plot(df_economics[features_to_flag[0]], label='% Changes in CCI', color='blue')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing trends for `CPI Value`\n",
    "plt.plot(df_economics[features_to_plot[1]], label='CPI', color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing trends for `CPI Rolling Percent Change`\n",
    "plt.plot(zero_line, color='black', linestyle='--')\n",
    "plt.plot(df_economics[features_to_flag[1]], label='% Changes in CPI', color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unemployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing trends for `Unemployment Rate (%)`\n",
    "plt.plot(df_economics[features_to_plot[2]], label='Unemployment Rate (%)', color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing trends for `Unemployment Rate Rolling Percent Change`\n",
    "plt.plot(zero_line, color='black', linestyle='--')\n",
    "plt.plot(df_economics[features_to_flag[2]], label='% Changes in Unemployment Rate', color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Economic Climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualizing total years classified in `Economic Climate`\n",
    "plt.barh(\n",
    "    y=df_economics['Economic Climate'].value_counts().index,\n",
    "    width=df_economics['Economic Climate'].value_counts()/12,\n",
    "    color=['darkblue', 'darkgreen'],\n",
    "    label=['26.42', '16.25']\n",
    ")\n",
    "plt.title(\n",
    "    'Years from 1981 to 2023 Classified as',\n",
    "    loc='left',\n",
    "    pad=15\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the index to recreate `Date` for later concatenation\n",
    "df_economics.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Data\n",
    "\n",
    "A combined dataset will need to be prepared for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging\n",
    "\n",
    "With both `df_movies` and `df_economics` prepared, the two datasets can be merged into one final working DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing\n",
    "\n",
    "The `df_movies` dataset will need to be set to a `Date` index, and the year and month will need to be extracted from the `Date` of the `df_economics` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a 'Date' for a datetime index\n",
    "df_movies['Date'] = pd.to_datetime({\n",
    "    'year': df_movies['released_year'],\n",
    "    'month': df_movies['released_month'],\n",
    "    'day': df_movies['released_day']\n",
    "})\n",
    "\n",
    "# Setting `Date` as index\n",
    "df_movies.set_index('Date', inplace=True)\n",
    "\n",
    "# Ensuring index is sorted with ascending dates\n",
    "df_movies.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a `Year` and `Month` for concatenation\n",
    "df_economics['Year'] = df_economics['Date'].dt.strftime('%Y').astype(int)\n",
    "df_economics['Month'] = df_economics['Date'].dt.strftime('%m').astype(int)\n",
    "\n",
    "# Renaming to `Year` and `Month` for concatenation\n",
    "df_movies.rename(columns={\n",
    "'released_year': 'Year',\n",
    "'released_month': 'Month'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging\n",
    "\n",
    "Generating the final record counts before and after merging the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming total records before concatenation\n",
    "records_total(df_economics)\n",
    "records_total(df_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining datasets through concatenation\n",
    "df_combined = pd.merge(df_economics, df_movies, how='left', on=['Year', 'Month'])\n",
    "\n",
    "# Confirming total records after concatenation\n",
    "records_total(df_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA\n",
    "\n",
    "Continuing EDA on the compiled DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target value\n",
    "\n",
    "Concatenating the two engineered target values from the `df_movies` dataset with the engineered target from the `df_economics` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the eventual `Target` for modeling\n",
    "df_combined['Success Indicator'] = df_combined['critical_success'].astype(str)\\\n",
    "                                   + ' ' +\\\n",
    "                                   df_combined['financial_success'].astype(str)\\\n",
    "                                   + ' ' +\\\n",
    "                                   df_combined['Economic Climate'].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming features\n",
    "\n",
    "For readability, certain features will be renamed prior to modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.rename(columns={\n",
    "    'vote_average': 'Rating'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing features and dataset\n",
    "\n",
    "Dropping uneeded features and removing `NaN` records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of features to drop\n",
    "cols_to_drop = [\n",
    "    'Date',\n",
    "    'CCI Rolling Mean',\n",
    "    'CPI Rolling Mean',\n",
    "    'Unemployment Rate (%) Rolling Mean',\n",
    "    'Year',\n",
    "    'Month',\n",
    "    'cast',\n",
    "    'original_language',\n",
    "    'director',\n",
    "    'writers',\n",
    "    'producers',\n",
    "    'popularity', \n",
    "    'critical_success',\n",
    "    'financial_success',\n",
    "    'release_date',\n",
    "    'released_day',\n",
    "    'production_countries',\n",
    "    'status',\n",
    "    'spoken_languages'\n",
    "]\n",
    "\n",
    "# Dropping unneeded features\n",
    "df_combined.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping `NaN` records\n",
    "df_combined.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming total records after concatenation\n",
    "print(f'Total records: {df_combined.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genres\n",
    "\n",
    "Reducing the `genres` feature to a single value for the purposes of visualizations\n",
    "\n",
    "*Note: Each instance of a given title will be present in* `df_comb_vis` *for each genre present in its original listing, allowing for more accurate aggregation across all genres present within the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming values of `genres`\n",
    "df_combined['genres'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a working copy of `df_combined` to preserve modeling integirty of dataset\n",
    "df_comb_vis = df_combined.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting strings in `genres` into lists\n",
    "df_comb_vis['genres'] = df_comb_vis['genres'].str.split(',')\n",
    "\n",
    "# Seperating records with multiple `genres` into individual records\n",
    "df_comb_vis = df_comb_vis.explode('genres')\n",
    "\n",
    "# Stripping white spaces from `genres`\n",
    "df_comb_vis['genres'] = df_comb_vis['genres'].str.strip()\n",
    "\n",
    "# Reconfirming values of `genres`\n",
    "df_comb_vis['genres'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming total unique `title` records same for `df_combined` and `df_comb_vis`\n",
    "df_combined['title'].unique().shape[0], df_comb_vis['title'].unique().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing features (additional)\n",
    "\n",
    "Dropping the final uneeded feature before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unneeded `Economic Climate`\n",
    "df_combined.drop(columns=['Economic Climate', 'id'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations\n",
    "\n",
    "Checking to see if any correlations exist among key features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[[\n",
    "    'roi',\n",
    "    'budget',\n",
    "    'revenue',\n",
    "    'Rating',\n",
    "    'CCI Value',\n",
    "    'CPI Value',\n",
    "    'Unemployment Rate (%)'\n",
    "]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While no strong correlations appear from this matrix, further exploration through visualizations shows some interesting connections between genres and economic climate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation and groupings\n",
    "\n",
    "Aggregating the `mean()` of `roi` and `sum()` of `revenue()` grouped by `genres`, `Economic Climate`, and both, then selecting subsets for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating the `mean()` of `roi` grouped on `genres`\n",
    "agg_roi_mean_genre = pd.DataFrame(\n",
    "    df_comb_vis.groupby('genres')['roi'].mean()\n",
    ").reset_index()\n",
    "\n",
    "# Viewing the aggregation\n",
    "agg_roi_mean_genre.sort_values(by='roi', ascending=False).style.format({'roi': '{:,.2f}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting all non-top-6 genres\n",
    "agg_roi_mean_genre_rmndr = agg_roi_mean_genre.loc[\n",
    "    (agg_roi_mean_genre['genres'] != 'TV Movie') &\n",
    "    (agg_roi_mean_genre['genres'] != 'Fantasy') &\n",
    "    (agg_roi_mean_genre['genres'] != 'Romance') &\n",
    "    (agg_roi_mean_genre['genres'] != 'Science Fiction') &\n",
    "    (agg_roi_mean_genre['genres'] != 'Comedy') &\n",
    "    (agg_roi_mean_genre['genres'] != 'Drama')\n",
    "]\n",
    "\n",
    "# Selecting the top 6 genres\n",
    "agg_roi_mean_genre_top_6_all = agg_roi_mean_genre.loc[\n",
    "    (agg_roi_mean_genre['genres'] == 'TV Movie') |\n",
    "    (agg_roi_mean_genre['genres'] == 'Fantasy') |\n",
    "    (agg_roi_mean_genre['genres'] == 'Romance') |\n",
    "    (agg_roi_mean_genre['genres'] == 'Science Fiction') |\n",
    "    (agg_roi_mean_genre['genres'] == 'Comedy') |\n",
    "    (agg_roi_mean_genre['genres'] == 'Drama')\n",
    "]\n",
    "\n",
    "# Selecting the top 6 genres, less `TV Movie`\n",
    "agg_roi_mean_genre_top_6_less = agg_roi_mean_genre.loc[\n",
    "    (agg_roi_mean_genre['genres'] == 'Fantasy') |\n",
    "    (agg_roi_mean_genre['genres'] == 'Romance') |\n",
    "    (agg_roi_mean_genre['genres'] == 'Science Fiction') |\n",
    "    (agg_roi_mean_genre['genres'] == 'Comedy') |\n",
    "    (agg_roi_mean_genre['genres'] == 'Drama')\n",
    "]\n",
    "\n",
    "# Selecting the top genre\n",
    "agg_roi_mean_genre_top_1 = agg_roi_mean_genre.loc[\n",
    "    (agg_roi_mean_genre['genres'] == 'TV Movie')\n",
    "]\n",
    "\n",
    "# Selecting the bottom 5 genres\n",
    "agg_roi_mean_genre_bot_5 = agg_roi_mean_genre.loc[\n",
    "    (agg_roi_mean_genre['genres'] == 'Western') |\n",
    "    (agg_roi_mean_genre['genres'] == 'Documentary') |\n",
    "    (agg_roi_mean_genre['genres'] == 'War') |\n",
    "    (agg_roi_mean_genre['genres'] == 'History') |\n",
    "    (agg_roi_mean_genre['genres'] == 'Crime')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating the `sum()` of `revenue` grouped on `genres`\n",
    "agg_rev_sum_genre = pd.DataFrame(\n",
    "    df_comb_vis.groupby('genres')['revenue'].sum()\n",
    ").reset_index()\n",
    "\n",
    "# Viewing the aggregation\n",
    "agg_rev_sum_genre.sort_values(by='revenue', ascending=False).style.format({'roi': '{:,.2f}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the non-top-3 genres\n",
    "agg_rev_sum_genre_rmndr = agg_rev_sum_genre.loc[\n",
    "    (agg_rev_sum_genre['genres'] != 'Adventure') &\n",
    "    (agg_rev_sum_genre['genres'] != 'Action') &\n",
    "    (agg_rev_sum_genre['genres'] != 'Comedy')\n",
    "]\n",
    "\n",
    "# Selecting the top 3 genres\n",
    "agg_rev_sum_genre_top_3 = agg_rev_sum_genre.loc[\n",
    "    (agg_rev_sum_genre['genres'] == 'Adventure') |\n",
    "    (agg_rev_sum_genre['genres'] == 'Action') |\n",
    "    (agg_rev_sum_genre['genres'] == 'Comedy')\n",
    "]\n",
    "\n",
    "# Selecting the bottom 3 genres\n",
    "agg_rev_sum_genre_bot_3 = agg_rev_sum_genre.loc[\n",
    "    (agg_rev_sum_genre['genres'] == 'TV Movie') |\n",
    "    (agg_rev_sum_genre['genres'] == 'Documentary') |\n",
    "    (agg_rev_sum_genre['genres'] == 'Western')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating the `mean()` of `roi` grouped on `Economic Climate`\n",
    "agg_roi_mean_economy = pd.DataFrame(\n",
    "    df_comb_vis.groupby('Economic Climate')['roi'].mean()\n",
    ").reset_index()\n",
    "\n",
    "# Viewing the aggregation\n",
    "agg_roi_mean_economy.sort_values(by='roi', ascending=False).style.format({'roi': '{:,.2f}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating the `sum()` of `revenue` grouped on `Economic Climate`\n",
    "agg_rev_most_economy = pd.DataFrame(\n",
    "    df_comb_vis.groupby('Economic Climate')['revenue'].sum()\n",
    ").reset_index()\n",
    "\n",
    "# Viewing the aggregation\n",
    "agg_rev_most_economy.sort_values(by='revenue', ascending=False).style.format({'roi': '{:,.2f}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating the `mean()` of `roi` grouped on `Economic Climate` and `genres`\n",
    "agg_roi_mean_both = pd.DataFrame(\n",
    "    df_comb_vis.groupby(['Economic Climate','genres'])['roi'].mean()\n",
    ").reset_index()\n",
    "\n",
    "# Viewing the aggregation\n",
    "agg_roi_mean_both.sort_values(by=['Economic Climate', 'roi'], ascending=False).style.format({'roi': '{:,.2f}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting top 5 overall genres\n",
    "agg_roi_mean_both_top_5_all = agg_roi_mean_both.loc[\n",
    "    (agg_roi_mean_both['genres'] == 'TV Movie') |\n",
    "    (agg_roi_mean_both['genres'] == 'Fantasy') |\n",
    "    (agg_roi_mean_both['genres'] == 'Romance') |\n",
    "    (agg_roi_mean_both['genres'] == 'Science Fiction') |\n",
    "    (agg_roi_mean_both['genres'] == 'Comedy')\n",
    "]\n",
    "\n",
    "# Selecting non-top-5-overall genres\n",
    "agg_roi_mean_both_rmndr_all = agg_roi_mean_both.loc[\n",
    "    (agg_roi_mean_both['genres'] != 'TV Movie') &\n",
    "    (agg_roi_mean_both['genres'] != 'Fantasy') &\n",
    "    (agg_roi_mean_both['genres'] != 'Romance') &\n",
    "    (agg_roi_mean_both['genres'] != 'Science Fiction') &\n",
    "    (agg_roi_mean_both['genres'] != 'Comedy')\n",
    "]\n",
    "\n",
    "# Selecting bottom 5 overall genres\n",
    "agg_roi_mean_both_bot_5_all = agg_roi_mean_both.loc[\n",
    "    (agg_roi_mean_both['genres'] == 'Documentary') |\n",
    "    (agg_roi_mean_both['genres'] == 'Western') |\n",
    "    (agg_roi_mean_both['genres'] == 'Western') |\n",
    "    (agg_roi_mean_both['genres'] == 'History') |\n",
    "    (agg_roi_mean_both['genres'] == 'War')\n",
    "]\n",
    "\n",
    "# Selecting only `TV Movie`\n",
    "agg_roi_mean_both_tv_only = agg_roi_mean_both.loc[\n",
    "    (agg_roi_mean_both['genres'] == 'TV Movie')\n",
    "]\n",
    "\n",
    "# Selecting only `Lean to Bad`\n",
    "agg_roi_mean_both_ltb = agg_roi_mean_both.loc[\n",
    "    (agg_roi_mean_both['Economic Climate'] == 'Lean to Bad')\n",
    "]\n",
    "\n",
    "# Selecting only `Comfortabl to Good`\n",
    "agg_roi_mean_both_ctg = agg_roi_mean_both.loc[\n",
    "    (agg_roi_mean_both['Economic Climate'] == 'Comfortable to Good')\n",
    "]\n",
    "\n",
    "# Selecting only `Comfortable to Good`, less `TV Movie` ***\n",
    "agg_roi_mean_both_ctg_tv = agg_roi_mean_both.loc[\n",
    "    (agg_roi_mean_both['Economic Climate'] == 'Comfortable to Good') &\n",
    "    (agg_roi_mean_both['genres'] != 'TV Movie')\n",
    "]\n",
    "\n",
    "# Selecting only `Lean to Bad` top 5 genres\n",
    "agg_roi_mean_both_top_5_ltb_setup = agg_roi_mean_both.loc[\n",
    "    (agg_roi_mean_both['genres'] == 'Fantasy') |\n",
    "    (agg_roi_mean_both['genres'] == 'Romance') |\n",
    "    (agg_roi_mean_both['genres'] == 'Science Fiction') |\n",
    "    (agg_roi_mean_both['genres'] == 'Comedy') |\n",
    "    (agg_roi_mean_both['genres'] == 'Drama')\n",
    "]\n",
    "agg_roi_mean_both_top_5_ltb = agg_roi_mean_both_top_5_ltb_setup.loc[\n",
    "    (agg_roi_mean_both_top_5_ltb_setup['Economic Climate'] == 'Lean to Bad')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating the `sum()` of `revenue` grouped on `Economic Climate` and `genres`\n",
    "agg_rev_most_both = pd.DataFrame(\n",
    "    df_comb_vis.groupby(['Economic Climate','genres'])['revenue'].sum()\n",
    ").reset_index()\n",
    "\n",
    "# Viewing the aggregation\n",
    "agg_rev_most_both.sort_values(by=['Economic Climate', 'revenue'], ascending=False).style.format({'roi': '{:,.2f}'})\n",
    "# agg_rev_most_both.sort_values(by='revenue', ascending=False).style.format({'roi': '{:,.2f}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting bottom 5 overall geners\n",
    "agg_rev_most_both_bot_5_all = agg_rev_most_both.loc[\n",
    "    (agg_rev_most_both['genres'] == 'TV Movie') |\n",
    "    (agg_rev_most_both['genres'] == 'Documentary') |\n",
    "    (agg_rev_most_both['genres'] == 'Western') |\n",
    "    (agg_rev_most_both['genres'] == 'War') |\n",
    "    (agg_rev_most_both['genres'] == 'History')\n",
    "]\n",
    "\n",
    "# Selecting only genres with better `revenue` in `Comfortable to Good`\n",
    "agg_rev_most_both_btr_ltb = agg_rev_most_both.loc[\n",
    "    (agg_rev_most_both['genres'] == 'Western') |\n",
    "    (agg_rev_most_both['genres'] == 'War')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizations\n",
    "\n",
    "Generating visualizations for aggregations of `df_comb_vis` subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mean()` of `roi` grouped by `genres`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ax = agg_roi_mean_genre.plot(\n",
    "    kind='bar', x='genres', y='roi',\n",
    "    figsize=(10, 6), legend=False, color='darkgreen'\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Mean ROI by Genre\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Mean ROI (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Format the y-axis to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "# plt.savefig('./Resources/images/Mean_ROI_by_Genre.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ax = agg_roi_mean_genre_rmndr.plot(\n",
    "    kind='bar', x='genres', y='roi',\n",
    "    figsize=(10, 6), legend=False, color='darkgreen'\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Mean ROI by Genre\\n(Non Top 6 Genres)\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Mean ROI (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Format the y-axis to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "# plt.savefig('./Resources/images/Mean_ROI_by_Genre_Non_Top_6.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ax = agg_roi_mean_genre_top_6_all.plot(\n",
    "    kind='bar', x='genres', y='roi',\n",
    "    figsize=(10, 6), legend=False, color='darkgreen'\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Mean ROI by Genre\\n (Top 6 Genres)\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Mean ROI (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Format the y-axis to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "# plt.savefig('./Resources/images/Mean_ROI_by_Genre_Top_6.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ax = agg_roi_mean_genre_top_6_less.plot(\n",
    "    kind='bar', x='genres', y='roi',\n",
    "    figsize=(10, 6), legend=False, color='darkgreen'\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Mean ROI by Genre\\n (Top 6 Genres, less 'TV Movie')\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Mean ROI (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Format the y-axis to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "# plt.savefig('./Resources/images/Mean_ROI_by_Genre_Top_6_no_TV.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ax = agg_roi_mean_genre_top_1.plot(\n",
    "    kind='bar', x='genres', y='roi',\n",
    "    figsize=(10, 6), legend=False, color='darkgreen'\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Mean ROI by Genre\\n ('TV Movie' Only)\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Mean ROI (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Format the y-axis to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "# plt.savefig('./Resources/images/Mean_ROI_by_Genre_TV_Only.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ax = agg_roi_mean_genre_bot_5.plot(\n",
    "    kind='bar', x='genres', y='roi',\n",
    "    figsize=(10, 6), legend=False, color='darkgreen'\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Mean ROI by Genre\\n (Bottom 5 Genres)\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Mean ROI (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Format the y-axis to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "# plt.savefig('./Resources/images/Mean_ROI_by_Genre_Bottom_5.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sum()` of `revenue` grouped by `genres`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ax = agg_rev_sum_genre.plot(\n",
    "    kind='bar', x='genres', y='revenue',\n",
    "    figsize=(10, 6), legend=False, color='#27408b'\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Total Revenue by Genre\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Reveue (USD)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Format the y-axis to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "# plt.savefig('./Resources/images/Sum_Revenue_by_Genre.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ax = agg_rev_sum_genre_rmndr.plot(\n",
    "    kind='bar', x='genres', y='revenue',\n",
    "    figsize=(10, 6), legend=False, color='#27408b'\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Total Revenue by Genre\\n(Non Top 3 Genres)\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Reveue (USD)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Format the y-axis to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "# plt.savefig('./Resources/images/Sum_Revenue_by_Genre_Non_Top_3.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ax = agg_rev_sum_genre_top_3.plot(\n",
    "    kind='bar', x='genres', y='revenue',\n",
    "    figsize=(10, 6), legend=False, color='#27408b'\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Total Revenue by Genre\\n(Top 3 Genres)\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Reveue (USD)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Format the y-axis to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "# plt.savefig('./Resources/images/Sum_Revenue_by_Genre_Top_3.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ax = agg_rev_sum_genre_bot_3.plot(\n",
    "    kind='bar', x='genres', y='revenue',\n",
    "    figsize=(10, 6), legend=False, color='#27408b'\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Total Revenue by Genre\\n(Bottom 3 Genres)\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Reveue (USD)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Format the y-axis to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "# plt.savefig('./Resources/images/Sum_Revenue_by_Genre_Bottom_3.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mean()` of `roi` groupbed by `Economic Climate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ax = agg_roi_mean_economy.plot(\n",
    "    kind='bar', x='Economic Climate', y='roi',\n",
    "    figsize=(10, 6), legend=False, color=['darkblue', 'darkred']\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Mean ROI by Economic Climate')\n",
    "plt.xlabel('Economic Climate')\n",
    "plt.ylabel('ROI (%)')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Format the y-axis to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "# plt.savefig('./Resources/images/Mean_ROI_by_Ecnomoic_Climate.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sum()` of `revenue` grouped by `Economic Climate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ax = agg_rev_most_economy.plot(\n",
    "    kind='bar', x='Economic Climate', y='revenue',\n",
    "    figsize=(10, 6), legend=False, color=['darkblue', 'darkred']\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Total Revenue by Economic Climate')\n",
    "plt.xlabel('Economic Climate')\n",
    "plt.ylabel('Total Revenue (USD)')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Format the y-axis to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "# plt.savefig('./Resources/images/Sum_Revenue_by_Economic_Climate.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mean()` of `roi` grouped by `Economic Climate` and  `genres`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivotting the table for plotting\n",
    "pivot_table = agg_roi_mean_both.pivot(index='genres', columns='Economic Climate', values='roi')\n",
    "\n",
    "# Plotting\n",
    "ax = pivot_table.plot(kind='bar', figsize=(14, 8), color=['blue', 'red'])\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Mean ROI by Genre and Economic Climate\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('ROI (%)')\n",
    "plt.legend(title='Economic Climate')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Formatting to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "# Displaying plot\n",
    "# plt.savefig('./Resources/images/Mean_ROI_by_Ecnomoic_Climate_and_Genre.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivotting the table for plotting\n",
    "pivot_table = agg_roi_mean_both_top_5_all.pivot(index='genres', columns='Economic Climate', values='roi')\n",
    "\n",
    "# Plotting\n",
    "ax = pivot_table.plot(kind='bar', figsize=(14, 8), color=['blue', 'red'])\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Mean ROI by Genre and Economic Climate\\n(Top 5 Overall Genres)\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('ROI (%)')\n",
    "plt.legend(title='Economic Climate')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Formatting to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "# Displaying plot\n",
    "# plt.savefig('./Resources/images/Mean_ROI_by_Ecnomoic_Climate_and_Genre_Top_5_Overall.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivotting the table for plotting\n",
    "pivot_table = agg_roi_mean_both_rmndr_all.pivot(index='genres', columns='Economic Climate', values='roi')\n",
    "\n",
    "# Plotting\n",
    "ax = pivot_table.plot(kind='bar', figsize=(14, 8), color=['blue', 'red'])\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Mean ROI by Genre and Economic Climate\\n(Non Top 5 Overall Genres)\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('ROI (%)')\n",
    "plt.legend(title='Economic Climate')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Formatting to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "# Displaying plot\n",
    "# plt.savefig('./Resources/images/Mean_ROI_by_Ecnomoic_Climate_and_Genre_Non_Top_5_Overall.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivotting the table for plotting\n",
    "pivot_table = agg_roi_mean_both_bot_5_all.pivot(index='genres', columns='Economic Climate', values='roi')\n",
    "\n",
    "# Plotting\n",
    "ax = pivot_table.plot(kind='bar', figsize=(14, 8), color=['blue', 'red'])\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Mean ROI by Genre and Economic Climate\\n(Bottom 5 Overall Genres)\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('ROI (%)')\n",
    "plt.legend(title='Economic Climate')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Formatting to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "# Displaying plot\n",
    "# plt.savefig('./Resources/images/Mean_ROI_by_Ecnomoic_Climate_and_Genre_Bottom_5_Overall.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivotting the table for plotting\n",
    "pivot_table = agg_roi_mean_both_tv_only.pivot(index='genres', columns='Economic Climate', values='roi')\n",
    "\n",
    "# Plotting\n",
    "ax = pivot_table.plot(kind='bar', figsize=(14, 8), color=['blue', 'red'])\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Mean ROI by Genre and Economic Climate\\n('TV Movie' only)\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('ROI (%)')\n",
    "plt.legend(title='Economic Climate')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Formatting to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "# Displaying plot\n",
    "# plt.savefig('./Resources/images/Mean_ROI_by_Ecnomoic_Climate_and_Genre_TV_Movie_Only.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ax = agg_roi_mean_both_ltb.plot(\n",
    "    kind='bar', x='genres', y='roi',\n",
    "    figsize=(10, 6), legend=False, color='red'\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Mean ROI by Genre and Economic Climate\\n(in 'Lean to Bad')\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Mean ROI (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Format the y-axis to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Displaying plot\n",
    "# plt.savefig('./Resources/images/Mean_ROI_by_Ecnomoic_Climate_and_Genre_Lean_to_Bad.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ax = agg_roi_mean_both_ctg.plot(\n",
    "    kind='bar', x='genres', y='roi',\n",
    "    figsize=(10, 6), legend=False, color='blue'\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Mean ROI by Genre and Economic Climate\\n(in 'Comfortable to Good')\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Mean ROI (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Format the y-axis to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Displaying plot\n",
    "# plt.savefig('./Resources/images/Mean_ROI_by_Ecnomoic_Climate_and_Genre_Comfortable_to_Good.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ax = agg_roi_mean_both_ctg_tv.plot(\n",
    "    kind='bar', x='genres', y='roi',\n",
    "    figsize=(10, 6), legend=False, color='blue'\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Mean ROI by Genre and Economic Climate\\n(in 'Comfortable to Good', less 'TV Movie')\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Mean ROI (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Format the y-axis to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Displaying plot\n",
    "# plt.savefig('./Resources/images/Mean_ROI_by_Ecnomoic_Climate_and_Genre_Better_Comfortable_to_Good.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ax = agg_roi_mean_both_top_5_ltb.plot(\n",
    "    kind='bar', x='genres', y='roi',\n",
    "    figsize=(10, 6), legend=False, color='red'\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Mean ROI by Genre and Economic Climate\\n(Top 5 Genres in 'Lean to Bad')\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Mean ROI (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Format the y-axis to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Displaying plot\n",
    "# plt.savefig('./Resources/images/Mean_ROI_by_Ecnomoic_Climate_and_Genre_Top_5_Lean_to_Bad.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sum()` of `revenue` grouped by `Economic Climate` and  `genres`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivotting the table for plotting\n",
    "pivot_table = agg_rev_most_both.pivot(index='genres', columns='Economic Climate', values='revenue')\n",
    "\n",
    "# Plotting\n",
    "ax = pivot_table.plot(kind='bar', figsize=(14, 8), color=['blue', 'red'])\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Total Revenue by Genre and Economic Climate')\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Revenue (USD)')\n",
    "plt.legend(title='Economic Climate')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Formatting to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "# Displaying plot\n",
    "# plt.savefig('./Resources/images/Sum_Revenue_by_Economic_Climate.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivotting the table for plotting\n",
    "pivot_table = agg_rev_most_both_bot_5_all.pivot(index='genres', columns='Economic Climate', values='revenue')\n",
    "\n",
    "# Plotting\n",
    "ax = pivot_table.plot(kind='bar', figsize=(14, 8), color=['blue', 'red'])\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Total Revenue by Genre and Economic Climate\\n(Bottom 5 Overall Genres)')\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Revenue (USD)')\n",
    "plt.legend(title='Economic Climate')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Formatting to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "# Displaying plot\n",
    "# plt.savefig('./Resources/images/Sum_Revenue_by_Economic_Climate_Bottom_5_Overall.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivotting the table for plotting\n",
    "pivot_table = agg_rev_most_both_btr_ltb.pivot(index='genres', columns='Economic Climate', values='revenue')\n",
    "\n",
    "# Plotting\n",
    "ax = pivot_table.plot(kind='bar', figsize=(14, 8), color=['blue', 'red'])\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Total Revenue by Genre and Economic Climate\\n(Better Revenue in 'Comfortable to Good' than 'Lean to Bad')\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Revenue (USD)')\n",
    "plt.legend(title='Economic Climate')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Formatting to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "# Displaying plot\n",
    "# plt.savefig('./Resources/images/Sum_Revenue_by_Economic_Climate_CtG_Better_LtB.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the lack of correlations, it appears as though both `roi` and `revenue` perform better during `Lean to Bad` economic conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train Test Splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining features and target\n",
    "X = df_combined.drop(columns='Success Indicator')\n",
    "y = df_combined['Success Indicator']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Scaling and Econding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Before proceeding to scaling and ecoding, a list of features for each needs to be established"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining features to scale\n",
    "col_to_scale = [\n",
    "    'CCI Value', 'CCI Rolling Percent Change', 'CPI Value',\n",
    "    'CPI Rolling Percent Change', 'Unemployment Rate (%)', \n",
    "    'Unemployment Rate Rolling Percent Change','Rating', 'vote_count',\n",
    "    'revenue','runtime','budget', 'roi'\n",
    "]\n",
    "\n",
    "# Defining features to encode\n",
    "col_to_encode = [\n",
    "    'CCI Rolling Percent Change Flag', 'CPI Rolling Percent Change Flag',\n",
    "    'Unemployment Rate Rolling Percent Change Flag', 'title', 'original_title',\n",
    "    'genres', 'production_companies'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "With the presence of positive and negative values within our economics dataset, the `StandardScalar()` was chosen over the `MinMaxScalar()` to retain the nature of our source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance for `StandardScalar()`\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fitting and transforming to `col_to_scale`\n",
    "X_train_scaled = scaler.fit_transform(X_train[col_to_scale])\n",
    "X_test_scaled = scaler.transform(X_test[col_to_scale])\n",
    "\n",
    "# Converting results to DF for later concatenation\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=col_to_scale)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=col_to_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "Encoding will be accomplished by two encoders; one for the encoded `X` data and one for the `y` target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OneHotEncoder()\n",
    "\n",
    "For our non-numeric features, `OneHotEncoder()` was selected as the most efficient method to encode our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance for `OneHotEncoder()` for `X_train[col_to_encode]`\n",
    "encoder_x = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fitting to `col_to_encode`\n",
    "encoder_x.fit(X_train[col_to_encode])\n",
    "\n",
    "# Transforming `X_train[col_to_encode]` and `X_test[col_to_encode]`\n",
    "X_train_encoded = encoder_x.transform(X_train[col_to_encode])\n",
    "X_test_encoded = encoder_x.transform(X_test[col_to_encode])\n",
    "\n",
    "# Converting results to DF for later concatenation\n",
    "X_train_encoded = pd.DataFrame(X_train_encoded, columns=encoder_x.get_feature_names_out())\n",
    "X_test_encoded = pd.DataFrame(X_test_encoded, columns=encoder_x.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation\n",
    "\n",
    "Combining the scaled and encoded featuers to a sing set each for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the `col_to_scale` with `col_to_encode` for `X_train` and `X_test`\n",
    "X_train = pd.concat([X_train_scaled, X_train_encoded], axis=1)\n",
    "X_test = pd.concat([X_test_scaled, X_test_encoded], axis=1)\n",
    "\n",
    "# Confirming total records after concatenation\n",
    "print(f'Total X records: {X_train.shape[0] + X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LabelEncoder()\n",
    "\n",
    "After poorer performance with `OneHotEncoder()`, the `LabelEncoder()` was selected for our `y` target to better serve the purposes of our modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance for `OneHotEncoder()` for `y_train`\n",
    "encoder_y = LabelEncoder()\n",
    "\n",
    "#Fitting\n",
    "encoder_y.fit(y_train.values.ravel())\n",
    "\n",
    "# Transforming `y_train` and `y_test`\n",
    "y_train_encoded = encoder_y.transform(y_train.values.ravel())\n",
    "y_test_encoded = encoder_y.transform(y_test.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modeling**\n",
    "\n",
    "Several models will be run to gauge which is best for predicting our `y` target of combined critical and financial success, as well as the economic climate of the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression()\n",
    "\n",
    "Serving as a baseline, all other modeled results will be compared to the performance of this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring an instace of `LogisticRegression()`\n",
    "logistic_regression_model = LogisticRegression(max_iter=500, solver='lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiting the model to the training data\n",
    "logistic_regression_model.fit(X_train, y_train_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions using the test data\n",
    "lr_predictions = logistic_regression_model.predict(X_test)\n",
    "\n",
    "# Reviewing predictions\n",
    "lr_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and testing scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying model scores\n",
    "print(f'Training score: {logistic_regression_model.score(X_train, y_train_encoded)}')\n",
    "print(f'Testing score: {logistic_regression_model.score(X_test, y_test_encoded)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy and precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the accuracy and precision scores\n",
    "print('Accuracy score:')\n",
    "print(accuracy_score(y_test_encoded, lr_predictions))\n",
    "print('\\nPrecision score:')\n",
    "print(precision_score(y_test_encoded, lr_predictions, average='weighted', zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Training score: 0.9809890815671163\n",
    "\n",
    "Testing score: 0.7773497688751926\n",
    "\n",
    "* Acc: 0.7773497688751926\n",
    "* Prc: 0.793423523704172\n",
    "\n",
    "**Decision:** A promising baseline for comparrison, if not prone to sensitivity to imbalanced classes such as those that exist in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model fit predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an untuned instance of `KNeighborsClassifier()`\n",
    "untuned_model = KNeighborsClassifier()\n",
    "\n",
    "# Fiting the model to the training data\n",
    "untuned_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Generating predictions using the test data\n",
    "untuned_y_test_pred = untuned_model.predict(X_test)\n",
    "\n",
    "# Displaying model scores\n",
    "print('Accuracy score:')\n",
    "print(accuracy_score(y_test_encoded, untuned_y_test_pred))\n",
    "print('\\nPrecision score:')\n",
    "print(precision_score(y_test_encoded, untuned_y_test_pred, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiatin and instance of `PCA` and declaring the number of PCA variables\n",
    "# to retain maximum variance\n",
    "pca = PCA(n_components=1, svd_solver='randomized')\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA model fit predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an untuned instance of `KNeighborsClassifier()`\n",
    "untuned_model = KNeighborsClassifier()\n",
    "\n",
    "# Fiting the model to the PCA training data\n",
    "untuned_model.fit(X_train_pca, y_train_encoded)\n",
    "\n",
    "# Generating predictions using the PCA test data\n",
    "untuned_y_test_pred = untuned_model.predict(X_test_pca)\n",
    "\n",
    "# Displaying model scores\n",
    "print('Accuracy score:')\n",
    "print(accuracy_score(y_test_encoded, untuned_y_test_pred))\n",
    "print('\\nPrecision score:')\n",
    "print(precision_score(y_test_encoded, untuned_y_test_pred, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding best K value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization\n",
    "\n",
    "Respect the noodle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KNN classfier to loop through different k values to find which has the highest accuracy.\n",
    "# Note: We use only odd numbers because we don't want any ties.\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for k in range(1, 40, 2):\n",
    "    loop_model = KNeighborsClassifier(n_neighbors=k)\n",
    "    loop_model.fit(X_train, y_train_encoded)\n",
    "    train_score = loop_model.score(X_train, y_train_encoded)\n",
    "    test_score = loop_model.score(X_test, y_test_encoded)\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "    print(f\"k: {k}, Train/Test Score: {train_score:.3f}/{test_score:.3f}\")\n",
    "    \n",
    "# Plotting the results\n",
    "plt.plot(range(1, 40, 2), train_scores, marker='o', label=\"training scores\")\n",
    "plt.plot(range(1, 40, 2), test_scores, marker=\"x\", label=\"testing scores\")\n",
    "plt.xlabel(\"k neighbors\")\n",
    "plt.ylabel(\"accuracy score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best K model fit predict\n",
    "\n",
    "`k=23` was chosen as it provides the best accuracy where the classifier is not overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an untuned instance of `KNeighborsClassifier()` using best `k` value\n",
    "loop_model = KNeighborsClassifier(n_neighbors=23)\n",
    "\n",
    "# Fiting the model to the training data\n",
    "loop_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Generating predictions using the test data\n",
    "loop_y_test_pred = loop_model.predict(X_test)\n",
    "\n",
    "# Displaying model scores\n",
    "print('Accuracy score:')\n",
    "print(accuracy_score(y_test_encoded, loop_y_test_pred))\n",
    "print('\\nPrecision score:')\n",
    "print(precision_score(y_test_encoded, loop_y_test_pred, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid \n",
    "\n",
    "The grid search below used to hyperparameter tune the KNN Classifier provided a `k` value of **17**, with an accurancy score of **0.6854206807964033**\n",
    "\n",
    "*Note: The code has been commented out code because it took 24min to run*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a grid search KNN classifier\n",
    "# grid_model = KNeighborsClassifier()\n",
    "\n",
    "# # Define the parameter grid tuned KNN classifier\n",
    "# param_grid = {'n_neighbors': list(range(1, 25, 2)),\n",
    "#             'weights': ['uniform', 'distance'],\n",
    "#             'leaf_size': [10, 50, 100, 500]\n",
    "# }\n",
    "\n",
    "# # Create a GridSearchCV model\n",
    "# grid = GridSearchCV(grid_model, param_grid, verbose=3)\n",
    "\n",
    "# # Fit the model by using the grid search estimator.\n",
    "# # This will take the KNN model and try each combination of parameters.\n",
    "# grid.fit(X_train, y_train_encoded)\n",
    "\n",
    "# # Best parameter and score\n",
    "# print(f\"Best k: {grid.best_params_['n_neighbors']}\")\n",
    "# print(f\"Best cross-validated accuracy: {grid.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Untuned:\n",
    "\n",
    "* Acc: 0.676040061633282\n",
    "* Prc: 0.6457813274107254\n",
    "\n",
    "PCA untuned:\n",
    "\n",
    "* Acc: 0.24807395993836673\n",
    "* Prc: 0.22671758347128645\n",
    "\n",
    "Loop at k=23:\n",
    "\n",
    "* Acc: 0.6818181818181818\n",
    "* Prc: 0.6316962265350805\n",
    "\n",
    "Best K: 17\n",
    "\n",
    "Cross-validated Acc: 0.6854206807964033\n",
    "\n",
    "**Decision:** A victim of the *\"Curse of Dimensionality\"*, the poor performance is likely due to the high-dimensional data causing the distance between data points to become meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring an `AdaBoostClassifier` model\n",
    "ada_model = AdaBoostClassifier(algorithm='SAMME', random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "ada_model.fit(X_train, y_train_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting with the model\n",
    "ada_pred = ada_model.predict(X_test)\n",
    "\n",
    "# Reviewing predictions\n",
    "ada_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and testing scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying model scores\n",
    "print(f'Training score: {ada_model.score(X_train, y_train_encoded)}')\n",
    "print(f'Testing score: {ada_model.score(X_test, y_test_encoded)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy and precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the accuracy and precision scores\n",
    "print('Accuracy score:')\n",
    "print(accuracy_score(y_test_encoded, ada_pred))\n",
    "print('\\nPrecision score:')\n",
    "print(precision_score(y_test_encoded, ada_pred, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Training score: 0.3892100192678227\n",
    "\n",
    "Testing score: 0.3709553158705701\n",
    "\n",
    "* Acc: 0.3709553158705701\n",
    "* Prc: 0.16592403022145988\n",
    "\n",
    "**Decision:** The noisy nature of our dataset may be causing this model to increase the weight of misclassifications, lending to its poor performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring an instace of `DecisionTreeClassifier()`\n",
    "dt_model = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiting the model to the training data\n",
    "dt_model.fit(X_train, y_train_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions using the test data\n",
    "dt_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Reviewing predictions\n",
    "dt_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and testing scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying model scores\n",
    "print(f'Training score: {dt_model.score(X_train, y_train_encoded)}')\n",
    "print(f'Testing score: {dt_model.score(X_test, y_test_encoded)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy and precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the accuracy and precision scores\n",
    "print('Accuracy score:')\n",
    "print(accuracy_score(y_test_encoded, dt_pred))\n",
    "print('\\nPrecision score:')\n",
    "print(precision_score(y_test_encoded, dt_pred, average='weighted', zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Training score: 1.0\n",
    "\n",
    "Testing score: 0.9899845916795069\n",
    "\n",
    "* Acc: 0.9899845916795069\n",
    "* Prc: 0.9920391410640323\n",
    "\n",
    "**Decision:** Suffering from overfitting, the results prove the addage of *\"too good to be true\"*, and the model would need an extreme amount of hyperparameter tuning to reduce the potential data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring an instace of `RandomForestClassifier()`\n",
    "random_forest_model = RandomForestClassifier(random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiting the model to the training data\n",
    "random_forest_model.fit(X_train, y_train_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions using the test data\n",
    "RFM_pred = random_forest_model.predict(X_test)\n",
    "\n",
    "# Reviewing predictions\n",
    "RFM_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and testing scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying model scores\n",
    "print(f'Training score: {random_forest_model.score(X_train, y_train_encoded)}')\n",
    "print(f'Testing score: {random_forest_model.score(X_test, y_test_encoded)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy and precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the accuracy and precision scores\n",
    "print('Accuracy score:')\n",
    "print(accuracy_score(y_test_encoded, RFM_pred))\n",
    "print('\\nPrecision score:')\n",
    "print(precision_score(y_test_encoded, RFM_pred, average='weighted', zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Training score: 1.0\n",
    "\n",
    "Testing score: 0.8035439137134053\n",
    "\n",
    "* Acc: 0.8035439137134053\n",
    "* Prc: 0.8360129287675775\n",
    "\n",
    "**Decision:** Tailored to large datasets with numerous variables, this model proved the most adept at meeting our classification needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "\n",
    "Before proceeding to scaling and ecoding, a list of features for each needs to be established"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining features to scale\n",
    "col_to_scale_lr = [\n",
    "    'CCI Value', 'CCI Rolling Percent Change', 'CPI Value',\n",
    "    'CPI Rolling Percent Change', 'Unemployment Rate (%)', \n",
    "    'Unemployment Rate Rolling Percent Change','Rating', 'vote_count',\n",
    "    'revenue','runtime','budget'\n",
    "]\n",
    "\n",
    "# Defining features to encode\n",
    "col_to_encode_lr = [\n",
    "    'CCI Rolling Percent Change Flag', 'CPI Rolling Percent Change Flag',\n",
    "    'Unemployment Rate Rolling Percent Change Flag', 'title', 'original_title',\n",
    "    'genres', 'production_companies'\n",
    "]\n",
    "\n",
    "col_for_X = [\n",
    "    'CCI Value', 'CCI Rolling Percent Change', 'CPI Value',\n",
    "    'CPI Rolling Percent Change', 'Unemployment Rate (%)', \n",
    "    'Unemployment Rate Rolling Percent Change','Rating', 'vote_count',\n",
    "    'revenue','runtime','budget', 'CCI Rolling Percent Change Flag',\n",
    "    'CPI Rolling Percent Change Flag', 'Unemployment Rate Rolling Percent Change Flag',\n",
    "    'title', 'original_title', 'genres', 'production_companies'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split\n",
    "\n",
    "Creating a subset of the previous `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining features and target\n",
    "X_lr = df_combined[col_for_X]\n",
    "y_lr = df_combined['roi'].values.reshape(-1, 1)\n",
    "\n",
    "# Confirming shapes\n",
    "print(\"Shape: \", X_lr.shape, y_lr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_lr, y_lr, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling\n",
    "\n",
    "With the presence of positive and negative values within our economics dataset, the `StandardScalar()` was chosen over the `MinMaxScalar()` to retain the nature of our source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance for `StandardScalar()`\n",
    "scaler_lr = StandardScaler()\n",
    "\n",
    "# Fitting and transforming to `col_to_scale`\n",
    "X_train_scaled_lr = scaler_lr.fit_transform(X_train_lr[col_to_scale_lr])\n",
    "X_test_scaled_lr = scaler_lr.transform(X_test_lr[col_to_scale_lr])\n",
    "\n",
    "# Converting results to DF for later concatenation\n",
    "X_train_scaled_lr = pd.DataFrame(X_train_scaled_lr, columns=col_to_scale_lr)\n",
    "X_test_scaled_lr = pd.DataFrame(X_test_scaled_lr, columns=col_to_scale_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OneHotEncoder()\n",
    "\n",
    "For our non-numeric features, `OneHotEncoder()` was selected as the most efficient method to encode our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance for `OneHotEncoder()` for `X_train[col_to_encode]`\n",
    "encoder_x_lr = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fitting to `col_to_encode`\n",
    "encoder_x_lr.fit(X_train_lr[col_to_encode_lr])\n",
    "\n",
    "# Transforming `X_train[col_to_encode]` and `X_test[col_to_encode]`\n",
    "X_train_encoded_lr = encoder_x_lr.transform(X_train_lr[col_to_encode_lr])\n",
    "X_test_encoded_lr = encoder_x_lr.transform(X_test_lr[col_to_encode_lr])\n",
    "\n",
    "# Converting results to DF for later concatenation\n",
    "X_train_encoded_lr = pd.DataFrame(X_train_encoded_lr, columns=encoder_x_lr.get_feature_names_out())\n",
    "X_test_encoded_lr = pd.DataFrame(X_test_encoded_lr, columns=encoder_x_lr.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation\n",
    "\n",
    "Combining the scaled and encoded featuers to a sing set each for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the `col_to_scale` with `col_to_encode` for `X_train` and `X_test`\n",
    "X_train_lr = pd.concat([X_train_scaled_lr, X_train_encoded_lr], axis=1)\n",
    "X_test_lr = pd.concat([X_test_scaled_lr, X_test_encoded_lr], axis=1)\n",
    "\n",
    "# Confirming total records after concatenation\n",
    "print(f'Total X records: {X_train_lr.shape[0] + X_test_lr.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Declaring an instace of `LinearRegression()`\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiting the model to the training data subset\n",
    "model.fit(X_train_lr, y_train_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions using the test data subset\n",
    "predicted_roi = model.predict(X_test_lr)\n",
    "\n",
    "# Reviewing predictions\n",
    "predicted_roi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying model scores\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test_lr, predicted_roi))\n",
    "print(\"R2 Score:\", r2_score(y_test_lr, predicted_roi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Mean Squared Error: 43298644487.0977\n",
    "\n",
    "R2 Score: -0.08725857589736674\n",
    "\n",
    "**Decision:** While showing a poor performance with our dataset, the results did guide our decision making towards choosing other, more applicable models to test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While no strong correlation seems to exist among the features used to create our `Success Indicator`, visualizations do indicate there are certain genres that perform drastically better in a `Lean to Bad` `Economic Climate`. Ultimately our hypothesis may be null, but our EDA and modeling proved well enough that further investigation may uncover more concrete findings.\n",
    "\n",
    "Based on our modelling, `LinearRegression()` and `AdaBoostClassifier` showed the poorest overall performance, while `DecisionTreeClassifier()` seemed to be overfitted to an extreme degree. Overall, `RandomForestClassifier()` showed the most promise for accurate and precise predictions for our `Success Indicator`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citations and Licenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citaions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Unemployment.csv**\n",
    "\n",
    "Economic Policy Institute, *State of Working America Data Library*, â€œUnemploymentâ€, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Licenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TMBD_all_movies.csv**\n",
    "\n",
    "Copyright 2024 __[Alan Vourc'h](https://www.kaggle.com/alanvourch)__\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "> __http://www.apache.org/licenses/LICENSE-2.0__\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CCI_OECD.csv** and **US_inflation_rates.csv**\n",
    "\n",
    "CCO: Public Domain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_dev",
   "language": "python",
   "name": "ai_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
