{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Data explanation placeholder\n",
    "\n",
    "(Talk about original two notebooks?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in data\n",
    "df_movies = pd.read_csv(\"./Resources/movies_data.csv\")\n",
    "df_economics = pd.read_csv(\"./Resources/economics_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Economics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a 'Date' for a datetime index\n",
    "df_movies['Date'] = pd.to_datetime({\n",
    "    'year': df_movies['released_year'],\n",
    "    'month': df_movies['released_month'],\n",
    "    'day': df_movies['released_day']\n",
    "})\n",
    "\n",
    "# Setting `Date` as index\n",
    "df_movies.set_index('Date', inplace=True)\n",
    "\n",
    "# Ensuring index is sorted with ascending dates\n",
    "df_movies.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a `Year` and `Month` for concatenation\n",
    "df_economics['Year'] = df_economics['Date'].str.slice(0,4).astype(int)\n",
    "df_economics['Month'] = df_economics['Date'].str.slice(5,7).astype(int)\n",
    "\n",
    "# Renaming to `Year` and `Month` for concatenation\n",
    "df_movies.rename(columns={\n",
    "'released_year': 'Year',\n",
    "'released_month': 'Month'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ecomonic records: 507\n",
      "Total movie records: 15363\n"
     ]
    }
   ],
   "source": [
    "# Confirming total records before concatenation\n",
    "print(f'Total ecomonic records: {df_economics.shape[0]}')\n",
    "print(f'Total movie records: {df_movies.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 12188\n"
     ]
    }
   ],
   "source": [
    "# Combining datasets through concatenation\n",
    "df_combined = pd.merge(df_economics, df_movies, how='left', on=['Year', 'Month'])\n",
    "\n",
    "# Confirming total records after concatenation\n",
    "print(f'Total records: {df_combined.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the eventual `Target` for modelling\n",
    "df_combined['Target'] = df_combined['critical_success'] + ' ' +\\\n",
    "                        df_combined['financial_success'] + ' ' +\\\n",
    "                        df_combined['Economic Climate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Target\n",
       "panned failure Lean to Bad                                    2200\n",
       "well liked failure Lean to Bad                                1897\n",
       "well liked failure Comfortable to Good                        1316\n",
       "panned failure Comfortable to Good                            1195\n",
       "alright failure Lean to Bad                                   1050\n",
       "alright failure Comfortable to Good                            753\n",
       "well liked excellent returns Lean to Bad                       709\n",
       "well liked excellent returns Comfortable to Good               527\n",
       "critical success failure Lean to Bad                           477\n",
       "well liked extraordinary returns Lean to Bad                   292\n",
       "critical success failure Comfortable to Good                   251\n",
       "well liked modest returns Lean to Bad                          221\n",
       "well liked extraordinary returns Comfortable to Good           201\n",
       "well liked moderate returns Lean to Bad                        193\n",
       "well liked modest returns Comfortable to Good                  164\n",
       "well liked moderate returns Comfortable to Good                139\n",
       "critical success excellent returns Lean to Bad                  76\n",
       "critical success excellent returns Comfortable to Good          65\n",
       "critical success extraordinary returns Lean to Bad              56\n",
       "critical success extraordinary returns Comfortable to Good      52\n",
       "panned broke even Lean to Bad                                   35\n",
       "alright excellent returns Lean to Bad                           28\n",
       "critical success modest returns Comfortable to Good             26\n",
       "alright excellent returns Comfortable to Good                   22\n",
       "critical success broke even Lean to Bad                         17\n",
       "critical success moderate returns Lean to Bad                   17\n",
       "alright moderate returns Lean to Bad                            16\n",
       "critical success moderate returns Comfortable to Good           16\n",
       "critical success broke even Comfortable to Good                 14\n",
       "critical success modest returns Lean to Bad                     14\n",
       "panned extraordinary returns Lean to Bad                        12\n",
       "panned broke even Comfortable to Good                           12\n",
       "panned excellent returns Lean to Bad                            12\n",
       "well liked broke even Lean to Bad                               11\n",
       "alright extraordinary returns Lean to Bad                       11\n",
       "panned moderate returns Lean to Bad                             10\n",
       "alright modest returns Comfortable to Good                      10\n",
       "panned modest returns Lean to Bad                                9\n",
       "well liked broke even Comfortable to Good                        9\n",
       "alright modest returns Lean to Bad                               9\n",
       "alright extraordinary returns Comfortable to Good                9\n",
       "panned excellent returns Comfortable to Good                     8\n",
       "alright moderate returns Comfortable to Good                     7\n",
       "panned moderate returns Comfortable to Good                      7\n",
       "panned modest returns Comfortable to Good                        6\n",
       "alright broke even Lean to Bad                                   3\n",
       "alright broke even Comfortable to Good                           2\n",
       "panned extraordinary returns Comfortable to Good                 2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of features to drop\n",
    "cols_to_drop = [\n",
    "    'Date',\n",
    "    'CCI Rolling Mean',\n",
    "    'CPI Rolling Mean',\n",
    "    'Unemployment Rate (%) Rolling Mean',\n",
    "    'Economic Climate',\n",
    "    'Year',\n",
    "    'Month',\n",
    "    'id',\n",
    "    'cast',\n",
    "    'original_language',\n",
    "    'director',\n",
    "    'writers',\n",
    "    'producers',\n",
    "    'popularity', \n",
    "    'critical_success',\n",
    "    'financial_success',\n",
    "    'release_date',\n",
    "    'released_day',\n",
    "    'production_countries',\n",
    "    'status',\n",
    "    'spoken_languages'\n",
    "]\n",
    "\n",
    "# Dropping unneeded features\n",
    "df_combined.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping `NaN` records\n",
    "df_combined.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10025, 20)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining columns to scale and encode\n",
    "col_to_scale = [\n",
    "    'CCI Value', 'CCI Rolling Percent Change', 'CPI Value',\n",
    "    'CPI Rolling Percent Change', 'Unemployment Rate (%)', \n",
    "    'Unemployment Rate Rolling Percent Change','vote_average', 'vote_count',\n",
    "    'revenue','runtime','budget', 'roi'\n",
    "]\n",
    "\n",
    "col_to_encode = [\n",
    "    'CCI Rolling Percent Change Flag', 'CPI Rolling Percent Change Flag',\n",
    "    'Unemployment Rate Rolling Percent Change Flag', 'title', 'original_title',\n",
    "    'genres', 'production_companies'\n",
    "]\n",
    "\n",
    "# Setup X and y variables\n",
    "X = df_combined.drop(columns='Target')\n",
    "y = df_combined['Target']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling and Econding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit(X_train[col_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.transform(X_train[col_to_scale])\n",
    "X_test_scaled = scaler.transform(X_test[col_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=scaler.get_feature_names_out())\n",
    "X_test_scaled = pd.DataFrame(X_train_scaled, columns=scaler.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.40 GiB for an array with shape (52626, 13781) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m encoder_y\u001b[38;5;241m.\u001b[39mfit(y_train\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Transform each column into numpy arrays\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m X_train_encoded \u001b[38;5;241m=\u001b[39m encoder_x\u001b[38;5;241m.\u001b[39mtransform(X_train[col_to_encode]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      9\u001b[0m X_test_encoded \u001b[38;5;241m=\u001b[39m encoder_x\u001b[38;5;241m.\u001b[39mtransform(X_test[col_to_encode]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     11\u001b[0m y_train_encoded \u001b[38;5;241m=\u001b[39m encoder_y\u001b[38;5;241m.\u001b[39mtransform(y_train\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:958\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    952\u001b[0m out \u001b[38;5;241m=\u001b[39m sparse\u001b[38;5;241m.\u001b[39mcsr_matrix(\n\u001b[0;32m    953\u001b[0m     (data, indices, indptr),\n\u001b[0;32m    954\u001b[0m     shape\u001b[38;5;241m=\u001b[39m(n_samples, feature_indices[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]),\n\u001b[0;32m    955\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m    956\u001b[0m )\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse_output:\n\u001b[1;32m--> 958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    960\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1050\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1049\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1050\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_toarray_args(order, out)\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:1267\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 5.40 GiB for an array with shape (52626, 13781) and data type float64"
     ]
    }
   ],
   "source": [
    "encoder_x = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "encoder_x.fit(X_train[col_to_encode].values.reshape(-1,1))\n",
    "\n",
    "encoder_y = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "encoder_y.fit(y_train.values.reshape(-1,1))\n",
    "\n",
    "# Transform each column into numpy arrays\n",
    "X_train_encoded = encoder_x.transform(X_train[col_to_encode].values.reshape(-1,1))\n",
    "X_test_encoded = encoder_x.transform(X_test[col_to_encode].values.reshape(-1,1))\n",
    "\n",
    "y_train_encoded = encoder_y.transform(y_train.values.reshape(-1,1))\n",
    "y_test_encoded = encoder_y.transform(y_test.values.reshape(-1,1))\n",
    "\n",
    "# Reorganize the numpy arrays into a DataFrame\n",
    "X_train_encoded = pd.DataFrame(X_train_encoded, columns=encoder_x.get_feature_names_out())\n",
    "X_test_encoded = pd.DataFrame(X_test_encoded, columns=encoder_x.get_feature_names_out())\n",
    "\n",
    "# Concatenate the encoded columns with the scaled columns\n",
    "X_train = pd.concat([X_train_scaled, X_train_encoded], axis=1)\n",
    "X_test = pd.concat([X_test_scaled, X_test_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Playtime!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eric's Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funda's Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalvin's Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Odele's Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peta's Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a logistic regression model.\n",
    "logistic_regression_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and save the logistic regression model using the training data\n",
    "df_combined_lr_model = logistic_regression_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "print(f\"Training Data Score: {df_combined_lr_model.score(X_train, y_train)}\")\n",
    "print(f\"Testing Data Score: {df_combined_lr_model.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from the logistic regression model using the test data\n",
    "lr_predictions = logistic_regression_model.predict(X_test)\n",
    "\n",
    "# Review the predictions\n",
    "testing_predections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the accuracy score for the test dataset.\n",
    "accuracy_score(y_test, testing_predections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the precision score for the test dataset.\n",
    "precision_score(y_test, testing_predections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vadim's Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
