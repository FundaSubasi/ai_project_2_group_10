{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reel Returns**\n",
    "#### *Machine Learning Insights into Movie Profitability*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report, balanced_accuracy_score, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing gdown (comment out if uneeded)\n",
    "%pip install gdown --quiet\n",
    "\n",
    "# Importing gdown\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerous factors contribue to the successes and failures within the film industry, both on critical and financial scales. Using a simplified view to focus on more generalized classifications, as well as to meet the puposes of our modelling, several contributing factors were chosen to highlight and predict each scale of success within our selected dataset. The features ultimately chosen were;\n",
    "\n",
    "* Vote Average (a given movie's rating from zero (0) to ten (10))\n",
    "* Vote Count\n",
    "* Revenue (total earnings in USD for a given movie)\n",
    "* Runtime\n",
    "* Budget (total expenses in USD to produce and promote a given movie)\n",
    "* Title\n",
    "* Original Title\n",
    "* Genres\n",
    "* Production Companies\n",
    "\n",
    "To create our `critical_success` indicator - a classification as to how well recieved a title was by fans and critics - we focused on the `vote_average` feature to create guidelines for ranges of scores.\n",
    "\n",
    "To engineer a `financial_success` indicator - a measure of what level of returns a tital produced - we used the percentage calculated as `budget` subtracted from `revenue`, then divided by `budget`, and compared the results to industry standard breakpoints.\n",
    "\n",
    "---\n",
    "\n",
    "The following dataset is courtesy of __[Kaggle](https://www.kaggle.com/)__.\n",
    "\n",
    "**__[TMDB_all_movies.csv](https://www.kaggle.com/datasets/alanvourch/tmdb-movies-daily-updates?select=TMDB_all_movies.csv)__**\n",
    "\n",
    "Per the dataset description;\n",
    "\n",
    "* This dataset was curated from __[The Movie Database](https://www.themoviedb.org/?language=en-US)__, and inspired by __[asaniczka](https://www.kaggle.com/asaniczka)__'s __[dataset](https://www.kaggle.com/datasets/asaniczka/tmdb-movies-dataset-2023-930k-movies)__\n",
    "* While updated daily, the dataset used in this notebook was downloaded on **7/7/2024**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring `url` and `output` for dataset\n",
    "url = 'https://drive.google.com/file/d/1Om73B4In4cHj0Rf6aIGOi3-8iXWbDAWg/view?usp=sharing'\n",
    "output = 'Resources/TMDB_all_movies.csv'\n",
    "\n",
    "# Downloading dataset\n",
    "gdown.download(url, output, fuzzy=True, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in dataset\n",
    "tmdb_data = pd.read_csv(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions\n",
    "\n",
    "The following function will be used code to help streamline the flow of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Universal functions\n",
    "\n",
    "Applicable to all datasets\n",
    "\n",
    "**Null Percentages**\n",
    "\n",
    "Calculating the percentage of null values, by feature, in a given dataset\n",
    "\n",
    "**Records Total**\n",
    "\n",
    "Confirming the total records for a given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to calculate the percentage of null values in\n",
    "# each feature of given DF\n",
    "def null_percentages(df):\n",
    "    return df.isnull().sum()/len(df)*100\n",
    "\n",
    "# Defining a function to display the total records for a given DF\n",
    "def records_total(df):\n",
    "    print(f'Total records: {df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Situational functions\n",
    "\n",
    "Applicable to some datasets or situations\n",
    "\n",
    "**Records Total by Feature Value**\n",
    "\n",
    "Confirming the total records for a given dataset based on a stated value for a single selected feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to display the total records for a given DF based on\n",
    "# a stated value for a single selected feature\n",
    "def records_total_feat(df, feature, value):\n",
    "    print(f\"Total selected records for '{feature}' of '{value}': {df.loc[df[feature] == value].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing `tmdb_data`\n",
    "tmdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying `null_percentages` to `tmdb_data`\n",
    "null_percentages(tmdb_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing features and dataset\n",
    "\n",
    "Dropping uneeded features and reducing dataset to domestic released movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of uneeded features\n",
    "movie_cols_to_drop = [\n",
    "    'imdb_id','overview', 'tagline', 'director_of_photography', 'music_composer'\n",
    "]\n",
    "\n",
    "# Dropping uneeded features\n",
    "tmdb_data.drop(columns=movie_cols_to_drop,inplace=True)\n",
    "\n",
    "# Reapplying `null_percentages` to `tmdb_data`\n",
    "null_percentages(tmdb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming values of `status`\n",
    "tmdb_data['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming values of `production_countries`\n",
    "tmdb_data['production_countries'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing dataset to only `Released` movies produced in `United States of America`\n",
    "df_movies = tmdb_data.loc[\n",
    "    (tmdb_data['production_countries'] == 'United States of America') &\n",
    "    (tmdb_data['status'] == 'Released')\n",
    "].copy()\n",
    "\n",
    "# Applying `null_percentages` to `us_tmdb_df`\n",
    "null_percentages(df_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering\n",
    "\n",
    "Engineering the two success target values, converting `rlease_date` to datetime, and reducing the `genres` feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critical success\n",
    "\n",
    "With a rating scale of zero (0) to ten (10), ranges can be established to break a given movie's critical success down by the following scale;\n",
    "\n",
    "* **0 to 2.5**: `panned`\n",
    "* **2.5 to 5**: `alright`\n",
    "* **5 to 7.5**: `well liked`\n",
    "* **7.5 to 10**: `critical success`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming values of `vote_average`\n",
    "df_movies['vote_average'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating bins to rate values of `vote_average`\n",
    "bins = [0, 2.5, 5, 7.5, 10]\n",
    "\n",
    "# Labelling bins\n",
    "critical_success = ['panned', 'alright', 'well liked', 'critical success']\n",
    "\n",
    "# Slicing the data and placing values in `critical_success`\n",
    "df_movies['critical_success'] = pd.cut(\n",
    "    df_movies['vote_average'], bins, labels=critical_success, include_lowest=True\n",
    ")\n",
    "\n",
    "# Confirming binned correctly\n",
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Financial success\n",
    "\n",
    "Classifying a given movie's financial success can be accomplished by generating a percentage to represent the return on investment (`roi`) calculated as;\n",
    "\n",
    "> ((`revenue`-`budget`)/`budget`) * 100\n",
    "\n",
    "The resulting value can then be compared to industry standard breakpoints to describe the folling classifications; \n",
    "\n",
    "* **Less than 0%**: `failure`\n",
    "* **Exactly 0%**: `broke even`\n",
    "* **Between 0% and 50%**: `modest returns`\n",
    "* **Between 50% and 100%**: `moderate returns`\n",
    "* **Between 100% and 500%**: `excellent returns`\n",
    "* **Over 500%**: `extraordinary returns`\n",
    "\n",
    "*Note: For the purposes of our modeling, only records with a `budget` NOT equal to zero (0) will be retained*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming total records with a `budget` of `0\n",
    "records_total_feat(df_movies, 'budget', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only records with a `budget` NOT equal to `0`\n",
    "df_movies = df_movies[df_movies['budget'] != 0].copy()\n",
    "\n",
    "# Confirming new total records with a `budget` of `0`\n",
    "records_total_feat(df_movies, 'budget', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating `roi` as described above\n",
    "df_movies['roi'] = (\n",
    "    (df_movies['revenue'] - df_movies['budget'])/df_movies['budget']\n",
    ") * 100\n",
    "\n",
    "# Confirming values of `roi`\n",
    "df_movies['roi'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming total records with a `roi` of `0`\n",
    "records_total_feat(df_movies, 'roi', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating bins to rate values of `roi`\n",
    "bins = [-float('inf'), 0, 50, 100, 500, float('inf')]\n",
    "\n",
    "# Labelling bins\n",
    "financial_success = [\n",
    "    'failure', 'modest returns', 'moderate returns',\n",
    "    'excellent returns', 'extraordinary returns'\n",
    "]\n",
    "\n",
    "# Slicing the data and placing values in `financial_success`\n",
    "df_movies['financial_success'] = pd.cut(\n",
    "    df_movies['roi'], bins, labels=financial_success, include_lowest=True\n",
    ")\n",
    "\n",
    "# Adding classification 'broke even' to `financial_success`\n",
    "df_movies['financial_success'] = df_movies['financial_success'].cat.add_categories('broke even')\n",
    "\n",
    "# Classifying where `roi` equals `0` as `broke even`\n",
    "df_movies.loc[df_movies['roi'] == 0, 'financial_success'] = 'broke even'\n",
    "\n",
    "# Confirming when `roi` is `0`, `financial_success` is 'broke even'\n",
    "df_movies.loc[df_movies['roi'] == 0, 'financial_success'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datetime values\n",
    "\n",
    "Converting `release_date` to a datetime value and extracting the year and month for later concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming dtype for `release_date`\n",
    "df_movies['release_date'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'release_date' to datetime\n",
    "df_movies['release_date'] = pd.to_datetime(\n",
    "    df_movies['release_date'], format='%Y-%m-%d', errors='coerce'\n",
    ")\n",
    "\n",
    "# Confirming conversion\n",
    "df_movies['release_date'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming records with `NaT` (Not a Time) values for `release_date`\n",
    "print(\n",
    "    'Total records where `release_date` has a `NaT` value: ' +\\\n",
    "    str(df_movies['release_date'].isna().sum())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping records where 'release_date' is `NaT`\n",
    "df_movies.dropna(subset=['release_date'], inplace=True)\n",
    "\n",
    "# Reconfirming records with `NaT` (Not a Time) values for `release_date` \n",
    "print(\n",
    "    'Total records where `release_date` has a `NaT` value: ' +\\\n",
    "    str(df_movies['release_date'].isna().sum())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting year, month, and day from 'release_date'\n",
    "df_movies['released_year'] = df_movies['release_date'].dt.year\n",
    "df_movies['released_month'] = df_movies['release_date'].dt.month\n",
    "df_movies['released_day'] = df_movies['release_date'].dt.day\n",
    "\n",
    "# Converting `released_year`, `released_month`, and `released_day` to integers\n",
    "df_movies['released_year'] = df_movies['released_year'].astype(int)\n",
    "df_movies['released_month'] = df_movies['released_month'].astype(int)\n",
    "df_movies['released_day'] = df_movies['released_day'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genres\n",
    "\n",
    "Reducing the `genres` feature to a single value\n",
    "\n",
    "*Note: In the cases where multiple genres are listed for a given movie, only the first listed genre will be retained*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming values of `genres`\n",
    "df_movies['genres'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting strings in `genres` into lists\n",
    "df_movies['genres'] = df_movies['genres'].str.split(',')\n",
    "\n",
    "# Seperating records with multiple `genres` into individual records\n",
    "df_movies = df_movies.explode('genres')\n",
    "\n",
    "# Stripping white spaces from `genres`\n",
    "df_movies['genres'] = df_movies['genres'].str.strip()\n",
    "\n",
    "# Reconfirming values of `genres`\n",
    "df_movies['genres'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying duplicate records in `df_movies` by `id`\n",
    "df_movies_duplicate = df_movies[df_movies.duplicated(subset=['id'], keep=False)]\n",
    "\n",
    "# Confirming total records of `df_movies_duplicate`\n",
    "records_total(df_movies_duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicate records by `id`\n",
    "df_movies = df_movies.drop_duplicates(subset=['id'], keep='first')\n",
    "\n",
    "# Identifying remaining duplicate records in `df_movies` by `id\n",
    "df_movies_duplicate = df_movies[df_movies.duplicated(subset=['id'], keep=False)]\n",
    "\n",
    "# Confirming total records of `df_movies_duplicate` and `df_movies`\n",
    "print('For `df_movies_duplicate`:')\n",
    "records_total(df_movies_duplicate)\n",
    "print('\\nFor `df_movies`:')\n",
    "records_total(df_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Economics Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While classifying economic states is a complex and nuanced issue, it is not unreasonable to draw more broad-strokes generalizations about a given timeframe based on more limited factors. To serve the purposes of our modeling, the following three factors were chosen to highlight the economic status at a given movie's release date;\n",
    "\n",
    "* Consumer Confidence Indicator (CCI)\n",
    "* Consumer Price Index (CPI)\n",
    "* Unemployment Rate\n",
    "\n",
    "These features stand as adequate datapoints to answer three respective questions;\n",
    "\n",
    "* How likely are people to be spending money?\n",
    "* How much do things cost when they do spend money?\n",
    "* How many people have jobs to earn money to spend?\n",
    "\n",
    "As detailed below, this information came as monthly measures over several decades. To create our `Economic Climate` indicator - a classification as to whether or not the economics of a given time were on the better side for consumers - we will need to calculate a rolling 12-month percent change in the mean of those monthly values in order to show if a given feature was on a positive or negative trend for the provided period.\n",
    "\n",
    "---\n",
    "\n",
    "The following datasets are courtesy of __[Kaggle](https://www.kaggle.com/)__.\n",
    "\n",
    "**__['CCI_OECD.csv'](https://www.kaggle.com/datasets/iqbalsyahakbar/cci-oecd)__**\n",
    "\n",
    "*renamed from `DP_LIVE_16112023095843236.csv`*\n",
    "\n",
    "Per the Organisation for Economic Co-operation and Development (OECD);\n",
    "\n",
    "* The CCI is an indication of developments for future households' consumption and saving based on expected financial situation, sentiment regarding the general economic situation, employment status, and capacity for savings\n",
    "* An indicator above `100` indicates an optimistic outlook and a greater likliehood to spend money over cautious saving\n",
    "* An indicator below `100` indicates a pessimistic outlook and both a higher likeliehood to save money and a lower tendency to consume\n",
    "\n",
    "**__['US_inflation_rates.csv'](https://www.kaggle.com/datasets/pavankrishnanarne/us-inflation-dataset-1947-present)__**\n",
    "\n",
    "Per the dataset description;\n",
    "\n",
    "* The CPI is a critical economic indicator for measuring the purchasing power of money over time, measuring the average change over time in the prices paid by urban consumers for goods and services\n",
    "* The CPI is the value at the end of the respective month\n",
    "\n",
    "---\n",
    "\n",
    "The following dataset is courtesy of the __[Economic Policy Institute’s (EPI) State of Working America Data Library](https://www.epi.org/data/)__.\n",
    "\n",
    "**__['Unemployment.csv'](https://www.epi.org/data/#?subject=unemp)__**\n",
    "\n",
    "Per EPI description;\n",
    "\n",
    "* Unemployment is the share of the labor force wihout a job\n",
    "* Monthly percentages calculated as a rolling 12-month average (mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in datasets\n",
    "df_unemp = pd.read_csv(\"./Resources/EPI Data Library - Unemployment.csv\")\n",
    "df_cci = pd.read_csv(\"./Resources/CCI_OECD.csv\")\n",
    "df_inflation = pd.read_csv(\"./Resources/US_inflation_rates.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions\n",
    "\n",
    "Since each dataset will need similar preprocessing, the following functions will be used to help streamline the flow of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Universal functions\n",
    "\n",
    "Applicable to all datasets\n",
    "\n",
    "**EDA routine**\n",
    "\n",
    "Labelling and displaying pertinant information about a given dataset for the purposes of expedited EDA\n",
    "\n",
    "**Copying datasets**\n",
    "\n",
    "Creating a working copy of a given dataset to preserve the original DF with unneeded features dropped\n",
    "\n",
    "**Renaming needed features**\n",
    "\n",
    "Renaming selected features for a given dataset\n",
    "\n",
    "**Rolling mean and mean percent change**\n",
    "\n",
    "Calculating the rolling 12-month mean and the rolling 12-month percent change for a given feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to display the `.describe()`, `.shape`, and `.dtypes`\n",
    "# for a given DF\n",
    "def eda_routine(df):\n",
    "    print('Describe:')\n",
    "    display(df.describe())\n",
    "    print(f'Shape: {df.shape}\\n')\n",
    "    print(f'Data types:')\n",
    "    display(df.dtypes)\n",
    "\n",
    "# Defining a function to copy a dataset with only the needed features\n",
    "def copy_df(df, features_to_keep):\n",
    "    df_copy = df[features_to_keep].copy()\n",
    "    return df_copy\n",
    "\n",
    "# Defining a function to rename needed features\n",
    "def rename_features(df, feature1, feature1new, feature2, feature2new):\n",
    "    df.rename(columns={\n",
    "        feature1: feature1new,\n",
    "        feature2: feature2new\n",
    "    }, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Defining a function to calculate the rolling 12-month means and percent changes\n",
    "# for a given feature\n",
    "def rolling_calcs(df, feature, feature_mean, feature_pct_chng):\n",
    "    df[feature_mean] = df[feature].rolling(window=12).mean()\n",
    "    df[feature_pct_chng] = df[feature_mean].pct_change(periods=12) * 100\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Situational functions\n",
    "\n",
    "Applicable to select datasets\n",
    "\n",
    "#### Datetime indexing\n",
    "\n",
    "Converting the feature containing the raw datetime information into a suitable datetime index\n",
    "\n",
    "*Cannot be used on `Unemployment` dataset*\n",
    "\n",
    "#### Removing '%'\n",
    "\n",
    "Removing the `'%'` from a given feature and converting the remaining `object` dtype to `float`\n",
    "\n",
    "*Specifically for `Unemployment` dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to set a `Date` feature as a datetime index\n",
    "def datetime_index(df, datetime_feature):\n",
    "    df[datetime_feature] = pd.to_datetime(df[datetime_feature])\n",
    "    df.set_index(datetime_feature, inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "# Defining a function to remove '%' and convert data `float`\n",
    "def convert_percentage(feature):\n",
    "    return float(feature.strip('%'))\n",
    "\n",
    "# Defining a function to apply `convert_percentage`\n",
    "def apply_percentage(df, feature):\n",
    "    df[feature] = df[feature].apply(convert_percentage)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CCI\n",
    "\n",
    "This dataset came with internaitonal records and uneeded features, so only records for US CCI will be retained. Once those records have been selected, the resulting DF will need to be prepared for concatenation with the remainined economic datasets. To do this, the `TIME` feature will be converted to datetime and set as the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing `df_cci`\n",
    "df_cci.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying `eda_routine` to `df_cci`\n",
    "eda_routine(df_cci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing dataset\n",
    "\n",
    "Reducing the dataset to only domestic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming values of `LOCATION`\n",
    "df_cci['LOCATION'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying domestic data from `df_cci` to `df_cci_us`\n",
    "df_cci_us = df_cci.loc[df_cci['LOCATION'] == 'USA'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying `df_cci_us` and dropping uneeded features\n",
    "df_cci_form = copy_df(df_cci_us, ['TIME', 'Value'])\n",
    "\n",
    "# Renamining retained features\n",
    "df_cci_form = rename_features(\n",
    "    df_cci_form, 'TIME', 'Date', 'Value', 'CCI Value'\n",
    ")\n",
    "\n",
    "# Converting `Date` to a datetime index\n",
    "df_cci_form = datetime_index(df_cci_form, 'Date')\n",
    "\n",
    "# Calculating rolling 12-month means and percent change in means\n",
    "df_cci_form = rolling_calcs(\n",
    "    df_cci_form, 'CCI Value', 'CCI Rolling Mean', 'CCI Rolling Percent Change'\n",
    ")\n",
    "\n",
    "# Confirming `df_cci_form` ready to concatenate\n",
    "display(df_cci_form.head())\n",
    "display(df_cci_form.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inflation\n",
    "\n",
    "Seeing as the dataset came with only the needed features, little will be needed to prepare the DF for concatenation with the other economic datasets. `date` will be converted to datetime and set as the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing `df_inflation`\n",
    "df_inflation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying `eda_routine` to `df_inflation`\n",
    "eda_routine(df_inflation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying `df_inflation` and dropping uneeded features\n",
    "df_inflation_form = copy_df(df_inflation, ['date', 'value'])\n",
    "\n",
    "# Renamining retained features\n",
    "df_inflation_form = rename_features(\n",
    "    df_inflation_form, 'date', 'Date', 'value', 'CPI Value'\n",
    ")\n",
    "\n",
    "# Converting `Date` to a datetime index\n",
    "df_inflation_form = datetime_index(df_inflation_form, 'Date')\n",
    "\n",
    "# Calculating rolling 12-month means and percent change in means\n",
    "df_inflation_form = rolling_calcs(\n",
    "    df_inflation_form,\n",
    "    'CPI Value',\n",
    "    'CPI Rolling Mean',\n",
    "    'CPI Rolling Percent Change'\n",
    ")\n",
    "\n",
    "# Confirming `df_inflation_form` ready to concatenate\n",
    "display(df_inflation_form.head())\n",
    "display(df_inflation_form.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unemployment\n",
    "\n",
    "This dataset came with uneeded features that will need to be dropped, as well as the needed features will need to be converted to `float`. Additionally, the `Date` feature will need to be converted to datetime and set to the index in preparation for concatenation with the other economic datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing `df_unemp`\n",
    "df_unemp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying `eda_routine` to `df_unemp`\n",
    "eda_routine(df_unemp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying defined functions (first pass)\n",
    "\n",
    "Given the nature of the `Date` feature in this dataset, the datetime indexing will need to be handled outside of the defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying `df_unemp` and dropping unneeded features\n",
    "df_unemp_form = copy_df(df_unemp, ['Date', 'All'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming and indexing\n",
    "\n",
    "This dataset only needed one feature, `All`, to be renmaned, therefore the `rename_features` defined function is not applicable\n",
    "\n",
    "Additionally, the `Date` feature will need to be engineered into a workable datetime feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the reatined feature\n",
    "df_unemp_form.rename(columns={'All': 'Unemployment Rate (%)'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary of Months\n",
    "month_map = {\n",
    "    'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
    "    'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
    "}\n",
    "\n",
    "# Mapping integer month values to `Date Month`\n",
    "df_unemp_form['Date Month'] = df_unemp_form['Date'].str.slice(0,3).map(month_map)\n",
    "\n",
    "# Slicing `Date Year`\n",
    "df_unemp_form['Date Year'] = df_unemp_form['Date'].str.slice(4,8)\n",
    "\n",
    "# Converting `Date` to datetime using `Date Month` and `Date Year`\n",
    "df_unemp_form['Date'] = pd.to_datetime({\n",
    "    'year': df_unemp_form['Date Year'],\n",
    "    'month': df_unemp_form['Date Month'],\n",
    "    'day': 1\n",
    "})\n",
    "\n",
    "# Dropping engineered features `Date Month` and `Date Year`\n",
    "df_unemp_form.drop(columns=['Date Month', 'Date Year'], inplace=True)\n",
    "\n",
    "# Setting `Date` as index\n",
    "df_unemp_form.set_index('Date', inplace=True)\n",
    "\n",
    "# Ensuring index is sorted with ascending dates\n",
    "df_unemp_form.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying defined functions (second pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying `apply_percentage` to `Unemployment Rate (%)`\n",
    "df_unemp_form = apply_percentage(df_unemp_form, 'Unemployment Rate (%)')\n",
    "\n",
    "# Calculating rolling 12-month means and percent change in means\n",
    "df_unemp_form = rolling_calcs(\n",
    "    df_unemp_form,\n",
    "    'Unemployment Rate (%)',\n",
    "    'Unemployment Rate (%) Rolling Mean',\n",
    "    'Unemployment Rate Rolling Percent Change',\n",
    ")\n",
    "\n",
    "# Confirming `df_unemp_form` ready to concatenate\n",
    "display(df_unemp_form.head())\n",
    "display(df_unemp_form.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Economics\n",
    "\n",
    "With all datasets set to a monthly datetime index, the relevent features can be combined into one DF, and any NaN records can be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconfirming total records and features for datasets\n",
    "print('CCI:')\n",
    "records_total(df_cci_form)\n",
    "print('\\nInflation (CPI):')\n",
    "records_total(df_inflation_form)\n",
    "print('\\nUnemployment:')\n",
    "records_total(df_unemp_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the economic datasets into `df_economics`\n",
    "df_economics = pd.concat(\n",
    "    [\n",
    "        df_cci_form,\n",
    "        df_inflation_form,\n",
    "        df_unemp_form\n",
    "    ], axis=1, join='outer'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `NaN` records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming total records and features\n",
    "df_economics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking total `NaN` records\n",
    "df_economics.isna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping `NaN` records\n",
    "df_economics.dropna(inplace=True)\n",
    "\n",
    "# Confirming remaining records\n",
    "records_total(df_economics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming final economic DF\n",
    "display(df_economics.head())\n",
    "display(df_economics.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering\n",
    "\n",
    "Engineering the economic target values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Economic Climate\n",
    "\n",
    "As stated, the goal is to create an indicator for `Economic Climate` based on broad-strokes observations of our datasets. Having calculated the rolling 12-month percent change for each feature - based off the rolling 12-month mean - we can look for a positive or negative change in values and flag the movement accordingly. From there, we can make the following simple statements;\n",
    "\n",
    "* For **CCI**, a positive change is \"good\", as it indicates an increase in the likelihood of consumers to spend money\n",
    "* For **CPI**, a negative change is \"good\", as it indicates a decrease in the costs for goods and services\n",
    "* For **Unemployment Rate**, a negative change is \"good\", as it indicates an incrase in the population active in the workforce\n",
    "\n",
    "Therefore, we can interpret movement contrary to those changes as \"bad\". With this simplified view of the features, we can draw a classification as follows;\n",
    "\n",
    "* If **at least two (2) features** have a \"good\" value, we can set `Economic Climate` to `Comfortable to Good`\n",
    "* If **at least two (2) features** have a \"bad\" value, we can set `Economic Climate` to `Lean to Bad`\n",
    "\n",
    "In this way, we can gague whether the ecnomic state at a given rlease date supports or disproves our hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming ranges and statistics of `df_economics`\n",
    "df_economics.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of features\n",
    "features_to_flag = [\n",
    "    'CCI Rolling Percent Change',\n",
    "    'CPI Rolling Percent Change',\n",
    "    'Unemployment Rate Rolling Percent Change'\n",
    "]\n",
    "\n",
    "# Looping through `features_to_flag` to assign `positive` and `negative` indicators\n",
    "for col in df_economics[features_to_flag].columns:\n",
    "    new_col = str(col) + ' Flag'\n",
    "    df_economics.loc[df_economics[col] > 0, new_col] = 'positive'\n",
    "    df_economics.loc[df_economics[col] <= 0, new_col] = 'negative'\n",
    "\n",
    "# Creating a of list flagged features\n",
    "flag_cols = [\n",
    "    'CCI Rolling Percent Change Flag',\n",
    "    'CPI Rolling Percent Change Flag',\n",
    "    'Unemployment Rate Rolling Percent Change Flag'\n",
    "]\n",
    "\n",
    "# Confirming indicators applied\n",
    "df_economics[flag_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of conditions and classifications\n",
    "conditions = [\n",
    "    ((df_economics[flag_cols[0]] == 'positive') &   # CCI = 'positive'/good\n",
    "    (df_economics[flag_cols[1]] == 'positive') &    # CPI = 'positive'/bad\n",
    "    (df_economics[flag_cols[2]] == 'positive'),     # Unemplyment = 'positive'/bad\n",
    "    'Lean to Bad'),\n",
    "    ((df_economics[flag_cols[0]] == 'positive') &   # CCI = 'positive'/good\n",
    "    (df_economics[flag_cols[1]] == 'positive') &    # CPI = 'positive'/bad\n",
    "    (df_economics[flag_cols[2]] == 'negative'),     # Unemplyment = 'negative'/good\n",
    "    'Comfortable to Good'),\n",
    "    ((df_economics[flag_cols[0]] == 'positive') &   # CCI = 'positive'/good\n",
    "    (df_economics[flag_cols[1]] == 'negative') &    # CPI = 'negative'/good\n",
    "    (df_economics[flag_cols[2]] == 'positive'),     # Unemplyment = 'positive'/bad\n",
    "    'Comfortable to Good'),\n",
    "    ((df_economics[flag_cols[0]] == 'negative') &   # CCI = 'negative'/bad\n",
    "    (df_economics[flag_cols[1]] == 'positive') &    # CPI = 'positive'/bad\n",
    "    (df_economics[flag_cols[2]] == 'positive'),     # Unemplyment = 'positive'/bad\n",
    "    'Lean to Bad'),\n",
    "    ((df_economics[flag_cols[0]] == 'negative') &   # CCI = 'negative'/bad\n",
    "    (df_economics[flag_cols[1]] == 'negative') &    # CPI = 'negative'/good\n",
    "    (df_economics[flag_cols[2]] == 'positive'),     # Unemplyment = 'positive'/bad\n",
    "    'Lean to Bad'),\n",
    "    ((df_economics[flag_cols[0]] == 'negative') &   # CCI = 'negative'/bad\n",
    "    (df_economics[flag_cols[1]] == 'positive') &    # CPI = 'positive'/bad\n",
    "    (df_economics[flag_cols[2]] == 'negative'),     # Unemplyment = 'negative'/good\n",
    "    'Lean to Bad'),\n",
    "    ((df_economics[flag_cols[0]] == 'positive') &   # CCI = 'positive'/good\n",
    "    (df_economics[flag_cols[1]] == 'negative') &    # CPI = 'negative'/good\n",
    "    (df_economics[flag_cols[2]] == 'negative'),     # Unemplyment = 'negative'/good\n",
    "    'Comfortable to Good'),\n",
    "    ((df_economics[flag_cols[0]] == 'negative') &   # CCI = 'negative'/bad\n",
    "    (df_economics[flag_cols[1]] == 'negative') &    # CPI = 'negative'/good\n",
    "    (df_economics[flag_cols[2]] == 'negative'),     # Unemplyment = 'negative'/good\n",
    "    'Comfortable to Good')\n",
    "]\n",
    "\n",
    "# Declaring `Economic Climate` with a `PLACEHOLDER` value\n",
    "df_economics['Economic Climate'] = 'PLACEHOLDER'\n",
    "\n",
    "# Applying conditions and classifications to `Economic Climate`\n",
    "for condition, classification in conditions:\n",
    "    df_economics.loc[condition, 'Economic Climate'] = classification\n",
    "\n",
    "# Confirming classifications applied\n",
    "df_economics['Economic Climate'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations\n",
    "\n",
    "Generating visualizations for `df_economics`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features and baselines\n",
    "\n",
    "Declaring some helpful lists and values for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of features\n",
    "features_to_plot = [\n",
    "    'CCI Value',\n",
    "    'CPI Value',\n",
    "    'Unemployment Rate (%)'\n",
    "]\n",
    "\n",
    "# Creating a value of `0` to show positive and negative values\n",
    "zero_line = pd.DataFrame({\n",
    "    'Date': df_economics.index,\n",
    "    'val': [x for x in 0*df_economics[features_to_flag[2]]]\n",
    "})\n",
    "zero_line.set_index('Date', inplace=True)\n",
    "\n",
    "# Creating a value of `100` to show break point for CCI\n",
    "hundred_line = pd.DataFrame({\n",
    "    'Date': df_economics.index,\n",
    "    'val': [x for x in (0*df_economics[features_to_flag[0]])+100]\n",
    "})\n",
    "hundred_line.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing trends for `CCI Value`\n",
    "plt.plot(hundred_line, color='black', linestyle='--')\n",
    "plt.plot(df_economics[features_to_plot[0]], label='CCI', color='blue')\n",
    "plt.title('Values above 100 (visualized)\\n indicate consumers more likely to spend vs save')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing trends for `CCI Rolling Percent Change`\n",
    "plt.plot(zero_line, color='black', linestyle='--')\n",
    "plt.plot(df_economics[features_to_flag[0]], label='% Changes in CCI', color='blue')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing trends for `CPI Value`\n",
    "plt.plot(df_economics[features_to_plot[1]], label='CPI', color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing trends for `CPI Rolling Percent Change`\n",
    "plt.plot(zero_line, color='black', linestyle='--')\n",
    "plt.plot(df_economics[features_to_flag[1]], label='% Changes in CPI', color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unemployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing trends for `Unemployment Rate (%)`\n",
    "plt.plot(df_economics[features_to_plot[2]], label='Unemployment Rate (%)', color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing trends for `Unemployment Rate Rolling Percent Change`\n",
    "plt.plot(zero_line, color='black', linestyle='--')\n",
    "plt.plot(df_economics[features_to_flag[2]], label='% Changes in Unemployment Rate', color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Economic Climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualizing total years classified in `Economic Climate`\n",
    "plt.barh(\n",
    "    y=df_economics['Economic Climate'].value_counts().index,\n",
    "    width=df_economics['Economic Climate'].value_counts()/12,\n",
    "    color=['darkblue', 'darkgreen'],\n",
    "    label=['26.42', '16.25']\n",
    ")\n",
    "plt.title(\n",
    "    'Years from 1981 to 2023 Classified as',\n",
    "    loc='left',\n",
    "    pad=15\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the index to recreate `Date` for later concatenation\n",
    "df_economics.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Data\n",
    "\n",
    "A combined dataset will need to be prepared for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging\n",
    "\n",
    "With both `df_movies` and `df_economics` prepared, the two datasets can be merged into one final working DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing\n",
    "\n",
    "The `df_movies` dataset will need to be set to a `Date` index, and the year and month will need to be extracted from the `Date` of the `df_economics` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a 'Date' for a datetime index\n",
    "df_movies['Date'] = pd.to_datetime({\n",
    "    'year': df_movies['released_year'],\n",
    "    'month': df_movies['released_month'],\n",
    "    'day': df_movies['released_day']\n",
    "})\n",
    "\n",
    "# Setting `Date` as index\n",
    "df_movies.set_index('Date', inplace=True)\n",
    "\n",
    "# Ensuring index is sorted with ascending dates\n",
    "df_movies.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a `Year` and `Month` for concatenation\n",
    "df_economics['Year'] = df_economics['Date'].dt.strftime('%Y').astype(int)\n",
    "df_economics['Month'] = df_economics['Date'].dt.strftime('%m').astype(int)\n",
    "\n",
    "# Renaming to `Year` and `Month` for concatenation\n",
    "df_movies.rename(columns={\n",
    "'released_year': 'Year',\n",
    "'released_month': 'Month'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging\n",
    "\n",
    "Generating the final record counts before and after merging the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming total records before concatenation\n",
    "records_total(df_economics)\n",
    "records_total(df_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining datasets through concatenation\n",
    "df_combined = pd.merge(df_economics, df_movies, how='left', on=['Year', 'Month'])\n",
    "\n",
    "# Confirming total records after concatenation\n",
    "records_total(df_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA\n",
    "\n",
    "Continuing EDA on the compiled DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target value\n",
    "\n",
    "Concatenating the two engineered target values from the `df_movies` dataset with the engineered target from the `df_economics` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the eventual `Target` for modeling\n",
    "df_combined['Target'] = df_combined['critical_success'].astype(str) + ' ' +\\\n",
    "                        df_combined['financial_success'].astype(str) + ' ' +\\\n",
    "                        df_combined['Economic Climate'].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing features and dataset\n",
    "\n",
    "Dropping uneeded features and removing `NaN` records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of features to drop\n",
    "cols_to_drop = [\n",
    "    'Date',\n",
    "    'CCI Rolling Mean',\n",
    "    'CPI Rolling Mean',\n",
    "    'Unemployment Rate (%) Rolling Mean',\n",
    "    'Year',\n",
    "    'Month',\n",
    "    'id',\n",
    "    'cast',\n",
    "    'original_language',\n",
    "    'director',\n",
    "    'writers',\n",
    "    'producers',\n",
    "    'popularity', \n",
    "    'critical_success',\n",
    "    'financial_success',\n",
    "    'release_date',\n",
    "    'released_day',\n",
    "    'production_countries',\n",
    "    'status',\n",
    "    'spoken_languages'\n",
    "]\n",
    "\n",
    "# Dropping unneeded features\n",
    "df_combined.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping `NaN` records\n",
    "df_combined.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming total records after concatenation\n",
    "print(f'Total records: {df_combined.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizations\n",
    "\n",
    "Generating visualizations for `df_combined`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating `roi` on `mean()` by `genres`\n",
    "agg_roi_mean_genre_most = pd.DataFrame(\n",
    "    df_combined.loc[\n",
    "        (df_combined['genres'] != 'TV Movie') &\n",
    "        (df_combined['genres'] != 'Comedy') &\n",
    "        (df_combined['genres'] != 'Action')\n",
    "    ].groupby('genres')['roi'].mean()\n",
    ").reset_index()\n",
    "\n",
    "# Plotting\n",
    "ax = agg_roi_mean_genre_most.plot(\n",
    "    kind='bar', x='genres', y='roi',\n",
    "    figsize=(10, 6), legend=False, color='darkred'\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Mean ROI by Genre\\n (Less 'TV Movie', 'Comedy', and 'Action')\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('MeanROI (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Format the y-axis to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating `roi` on `mean()` by `genres`\n",
    "agg_roi_mean_genre_top_3 = pd.DataFrame(\n",
    "    df_combined.loc[\n",
    "        (df_combined['genres'] == 'Comedy') |\n",
    "        (df_combined['genres'] == 'Action')\n",
    "    ].groupby('genres')['roi'].mean()\n",
    ").reset_index()\n",
    "\n",
    "# Plotting\n",
    "ax = agg_roi_mean_genre_top_3.plot(\n",
    "    kind='bar', x='genres', y='roi',\n",
    "    figsize=(10, 6), legend=False, color='darkred'\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Mean ROI by Genre\\n ('Comedy' and 'Action' Only)\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('MeanROI (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Format the y-axis to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating `roi` on `mean()` by `genres`\n",
    "agg_roi_mean_genre_top_1 = pd.DataFrame(\n",
    "    df_combined.loc[\n",
    "        (df_combined['genres'] == 'TV Movie')\n",
    "    ].groupby('genres')['roi'].mean()\n",
    ").reset_index()\n",
    "\n",
    "# Plotting\n",
    "ax = agg_roi_mean_genre_top_1.plot(\n",
    "    kind='bar', x='genres', y='roi',\n",
    "    figsize=(10, 6), legend=False, color='darkred'\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Mean ROI by Genre\\n ('TV Movie' Only)\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('MeanROI (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Format the y-axis to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating `revenue` on `sum()` by `genres`\n",
    "agg_rev_sum_genre = pd.DataFrame(\n",
    "    df_combined.groupby('genres')['revenue'].sum()\n",
    ").reset_index()\n",
    "\n",
    "# Plotting\n",
    "ax = agg_rev_sum_genre.plot(\n",
    "    kind='bar', x='genres', y='revenue',\n",
    "    figsize=(10, 6), legend=False, color='green'\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Total Revenue by Genre')\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Total Revenue (USD)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Format the y-axis to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating `roi` on `mean()` by `Economic Climate`\n",
    "agg_roi_mean_economy = pd.DataFrame(\n",
    "    df_combined.groupby('Economic Climate')['roi'].mean()\n",
    ").reset_index()\n",
    "\n",
    "# Plotting\n",
    "ax = agg_roi_mean_economy.plot(\n",
    "    kind='bar', x='Economic Climate', y='roi',\n",
    "    figsize=(10, 6), legend=False, color=['darkblue', 'darkred']\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Mean ROI by Economic Climate')\n",
    "plt.xlabel('Economic Climate')\n",
    "plt.ylabel('ROI (%)')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Format the y-axis to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating `revenue` on `sum()` by `Economic Climate`\n",
    "agg_rev_sum_economy = pd.DataFrame(\n",
    "    df_combined.groupby('Economic Climate')['revenue'].sum()\n",
    ").reset_index()\n",
    "\n",
    "# Plotting\n",
    "ax = agg_rev_sum_economy.plot(\n",
    "    kind='bar', x='Economic Climate', y='revenue',\n",
    "    figsize=(10, 6), legend=False, color=['darkblue', 'darkred']\n",
    ")\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Total Revenue by Economic Climate')\n",
    "plt.xlabel('Economic Climate')\n",
    "plt.ylabel('Total Revenue (USD)')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Format the y-axis to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating `roi` on `mean()` by `Economic Climate` and `genres`\n",
    "agg_roi_mean_most = pd.DataFrame(\n",
    "    df_combined.loc[\n",
    "        (df_combined['genres'] != 'TV Movie') &\n",
    "        (df_combined['genres'] != 'Comedy') &\n",
    "        (df_combined['genres'] != 'Action')\n",
    "    ].groupby(['Economic Climate', 'genres'])['roi'].mean()\n",
    ").reset_index()\n",
    "\n",
    "# Pivotting the table for plotting\n",
    "pivot_table = agg_roi_mean_most.pivot(index='genres', columns='Economic Climate', values='roi')\n",
    "\n",
    "# Plotting\n",
    "ax = pivot_table.plot(kind='bar', figsize=(14, 8), color=['blue', 'red'])\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Mean ROI by Genre and Economic Climate\\n (Less 'TV Movie', 'Comedy', and 'Action')\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('ROI (%)')\n",
    "plt.legend(title='Economic Climate')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Formatting to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "# Displaying plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating `roi` on `mean()` by `Economic Climate` and `genres`\n",
    "agg_roi_mean_both_top_3 = pd.DataFrame(\n",
    "    df_combined.loc[\n",
    "        (df_combined['genres'] == 'Comedy') |\n",
    "        (df_combined['genres'] == 'Action')\n",
    "    ].groupby(['Economic Climate', 'genres'])['roi'].mean()\n",
    ").reset_index()\n",
    "\n",
    "# Pivotting the table for plotting\n",
    "pivot_table = agg_roi_mean_both_top_3.pivot(index='genres', columns='Economic Climate', values='roi')\n",
    "\n",
    "# Plotting\n",
    "ax = pivot_table.plot(kind='bar', figsize=(14, 8), color=['blue', 'red'])\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Mean ROI by Genre and Economic Climate\\n ('Comedy' and 'Action' Only)\")\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('ROI (%)')\n",
    "plt.legend(title='Economic Climate')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Formatting to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "# Displaying plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating `roi` on `mean()` by `Economic Climate` and `genres`\n",
    "agg_roi_mean_both_top_1_good = pd.DataFrame(\n",
    "    df_combined.loc[\n",
    "        (df_combined['genres'] == 'TV Movie') &\n",
    "        (df_combined['Economic Climate'] == 'Comfortable to Good')\n",
    "    ].groupby(['Economic Climate', 'genres'])['roi'].mean()\n",
    ").reset_index()\n",
    "\n",
    "# Aggregating `roi` on `mean()` by `Economic Climate` and `genres`\n",
    "agg_roi_mean_both_top_1_bad = pd.DataFrame(\n",
    "    df_combined.loc[\n",
    "        (df_combined['genres'] == 'TV Movie') &\n",
    "        (df_combined['Economic Climate'] == 'Lean to Bad')\n",
    "    ].groupby(['Economic Climate', 'genres'])['roi'].mean()\n",
    ").reset_index()\n",
    "\n",
    "# Pivotting the tables for plotting\n",
    "pivot_table_good = agg_roi_mean_both_top_1_good.pivot(index='genres', columns='Economic Climate', values='roi')\n",
    "pivot_table_bad = agg_roi_mean_both_top_1_bad.pivot(index='genres', columns='Economic Climate', values='roi')\n",
    "                                                \n",
    "# Creating subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))\n",
    "\n",
    "# Plotting for 'Lean to Bad'\n",
    "ax1 = pivot_table_bad.plot(kind='bar', ax=axes[0], color='red')\n",
    "ax1.set_title(\"Mean ROI by Genre in 'Lean to Bad' Economic Climate\\n ('TV Movie' Only)\")\n",
    "ax1.set_xlabel('Genres')\n",
    "ax1.set_ylabel('ROI (%)')\n",
    "ax1.legend(title='Economic Climate')\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "ax1.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "# Plotting for 'Comfortable to Good'\n",
    "ax2 = pivot_table_good.plot(kind='bar', ax=axes[1], color='blue')\n",
    "ax2.set_title(\"Mean ROI by Genre in 'Comfortable to Good' Economic Climate\\n ('TV Movie' Only)\")\n",
    "ax2.set_xlabel('Genres')\n",
    "ax2.set_ylabel('ROI (%)')\n",
    "ax2.legend(title='Economic Climate')\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\n",
    "ax2.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "# Adjusting layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Displaying plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating `revenue` on `sum()` by `Economic Climate` and `genres`\n",
    "agg_rev_sum_both = pd.DataFrame(\n",
    "    df_combined.groupby(['Economic Climate', 'genres'])['revenue'].sum()\n",
    ").reset_index()\n",
    "\n",
    "# Pivotting the table for plotting\n",
    "pivot_table = agg_rev_sum_both.pivot(index='genres', columns='Economic Climate', values='revenue')\n",
    "\n",
    "# Plotting\n",
    "ax = pivot_table.plot(kind='bar', figsize=(14, 8), color=['blue', 'red'])\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Total Revenue by Genre and Economic Climate')\n",
    "plt.xlabel('Genres')\n",
    "plt.ylabel('Revenue (USD)')\n",
    "plt.legend(title='Economic Climate')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Formatting to avoid scientific notation\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "# Displaying plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing features\n",
    "\n",
    "Dropping the final uneeded feature before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unneeded `Economic Climate`\n",
    "df_combined.drop(columns=['Economic Climate'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train Test Splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup X and y variables\n",
    "X = df_combined.drop(columns='Target')\n",
    "y = df_combined['Target']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Scaling and Econding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining columns to scale and encode\n",
    "col_to_scale = [\n",
    "    'CCI Value', 'CCI Rolling Percent Change', 'CPI Value',\n",
    "    'CPI Rolling Percent Change', 'Unemployment Rate (%)', \n",
    "    'Unemployment Rate Rolling Percent Change','vote_average', 'vote_count',\n",
    "    'revenue','runtime','budget', 'roi'\n",
    "]\n",
    "\n",
    "col_to_encode = [\n",
    "    'CCI Rolling Percent Change Flag', 'CPI Rolling Percent Change Flag',\n",
    "    'Unemployment Rate Rolling Percent Change Flag', 'title', 'original_title',\n",
    "    'genres', 'production_companies'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance for `StandardScalar()`\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fitting and transforming to `col_to_scale`\n",
    "X_train_scaled = scaler.fit_transform(X_train[col_to_scale])\n",
    "X_test_scaled = scaler.transform(X_test[col_to_scale])\n",
    "\n",
    "# Converting results to DF for later concatenation\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=col_to_scale)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=col_to_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance for `OneHotEncoder()` for `X_train[col_to_encode]`\n",
    "encoder_x = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fitting to `col_to_encode`\n",
    "encoder_x.fit(X_train[col_to_encode])\n",
    "\n",
    "# Creating an instance for `OneHotEncoder()` for `y_train`\n",
    "encoder_y = LabelEncoder()\n",
    "\n",
    "#Fitting\n",
    "encoder_y.fit(y_train.values.ravel())\n",
    "\n",
    "# Transforming `X_train[col_to_encode]` and `X_test[col_to_encode]`\n",
    "X_train_encoded = encoder_x.transform(X_train[col_to_encode])\n",
    "X_test_encoded = encoder_x.transform(X_test[col_to_encode])\n",
    "\n",
    "# Transforming `y_train` and `y_test`\n",
    "y_train_encoded = encoder_y.transform(y_train.values.ravel())\n",
    "y_test_encoded = encoder_y.transform(y_test.values.ravel())\n",
    "\n",
    "# Converting results to DF for later concatenation\n",
    "X_train_encoded = pd.DataFrame(X_train_encoded, columns=encoder_x.get_feature_names_out())\n",
    "X_test_encoded = pd.DataFrame(X_test_encoded, columns=encoder_x.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the `col_to_scale` with `col_to_encode` for `X_train` and `X_test`\n",
    "X_train = pd.concat([X_train_scaled, X_train_encoded], axis=1)\n",
    "X_test = pd.concat([X_test_scaled, X_test_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming total records after concatenation\n",
    "print(f'Total X records: {X_train.shape[0] + X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Playtime!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eric's Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funda's Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalvin's Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a untuned KNN classifier\n",
    "untuned_model = KNeighborsClassifier()\n",
    "\n",
    "## Train the untuned model\n",
    "untuned_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Check the model's accuracy on the test set\n",
    "untuned_y_test_pred = untuned_model.predict(X_test)\n",
    "print(accuracy_score(y_test_encoded, untuned_y_test_pred))\n",
    "print(precision_score(y_test_encoded, untuned_y_test_pred, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the PCA instance and declare the number of PCA variables to retain maximum variance\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a untuned KNN classifier\n",
    "untuned_model = KNeighborsClassifier()\n",
    "\n",
    "## Train the untuned model\n",
    "untuned_model.fit(X_train_pca, y_train_encoded)\n",
    "\n",
    "# Check the model's accuracy on the test set\n",
    "untuned_y_test_pred = untuned_model.predict(X_test_pca)\n",
    "print(accuracy_score(y_test_encoded, untuned_y_test_pred))\n",
    "print(precision_score(y_test_encoded, untuned_y_test_pred, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KNN classfier to loop through different k values to find which has the highest accuracy.\n",
    "# Note: We use only odd numbers because we don't want any ties.\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for k in range(1, 40, 2):\n",
    "    loop_model = KNeighborsClassifier(n_neighbors=k)\n",
    "    loop_model.fit(X_train, y_train_encoded)\n",
    "    train_score = loop_model.score(X_train, y_train_encoded)\n",
    "    test_score = loop_model.score(X_test, y_test_encoded)\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "    print(f\"k: {k}, Train/Test Score: {train_score:.3f}/{test_score:.3f}\")\n",
    "    \n",
    "# Plot the results\n",
    "plt.plot(range(1, 40, 2), train_scores, marker='o', label=\"training scores\")\n",
    "plt.plot(range(1, 40, 2), test_scores, marker=\"x\", label=\"testing scores\")\n",
    "plt.xlabel(\"k neighbors\")\n",
    "plt.ylabel(\"accuracy score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the best k, and refit the KNN classifier by using that k value.\n",
    "# Note that k=23 provides the best accuracy where the classifier is not overfitting.\n",
    "loop_model = KNeighborsClassifier(n_neighbors=23)\n",
    "loop_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Check the model's accuracy on the test set\n",
    "loop_y_test_pred = loop_model.predict(X_test)\n",
    "print(accuracy_score(y_test_encoded, loop_y_test_pred))\n",
    "print(precision_score(y_test_encoded, loop_y_test_pred, average='weighted', zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The grid search below used to hyperparameter tune the KNN Classifier provided\n",
    "a k value = 1, with an accurancy score of .578.\n",
    "The code has been commented out code because it took 24min to run.\n",
    "'''\n",
    "# # Create a grid search KNN classifier\n",
    "# grid_model = KNeighborsClassifier()\n",
    "\n",
    "# # Define the parameter grid tuned KNN classifier\n",
    "# param_grid = {'n_neighbors': list(range(1, 25, 2)),\n",
    "#             'weights': ['uniform', 'distance'],\n",
    "#             'leaf_size': [10, 50, 100, 500]\n",
    "# }\n",
    "\n",
    "# # Create a GridSearchCV model\n",
    "# grid = GridSearchCV(grid_model, param_grid, verbose=3)\n",
    "\n",
    "# # Fit the model by using the grid search estimator.\n",
    "# # This will take the KNN model and try each combination of parameters.\n",
    "# grid.fit(X_train, y_train_encoded)\n",
    "\n",
    "# # Best parameter and score\n",
    "# print(f\"Best k: {grid.best_params_['n_neighbors']}\")\n",
    "# print(f\"Best cross-validated accuracy: {grid.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Odele's Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peta-Gaye's LR modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a logistic regression model.\n",
    "logistic_regression_model = LogisticRegression(max_iter=500, solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and save the logistic regression model using the training data\n",
    "df_combined_lr_model = logistic_regression_model.fit(X_train, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from the logistic regression model using the test data\n",
    "lr_predictions = logistic_regression_model.predict(X_test)\n",
    "\n",
    "# Review the predictions\n",
    "lr_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the accuracy score for the test dataset.\n",
    "accuracy_score(y_test_encoded, lr_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the precision score for the test dataset.\n",
    "precision_score(y_test_encoded, lr_predictions, average='weighted', zero_division=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End Peta-Gaye's LR modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eric's AdaBoost modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring an `AdaBoostClassifier` model\n",
    "ada_model = AdaBoostClassifier(algorithm='SAMME', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "ada_model.fit(X_train, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying model scores\n",
    "print(f'Training score: {ada_model.score(X_train, y_train_encoded)}')\n",
    "print(f'Testing score: {ada_model.score(X_test, y_test_encoded)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting with the model\n",
    "ada_pred = ada_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the accuracy score for the test dataset\n",
    "accuracy_score(y_test_encoded, ada_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the precision score for the test dataset\n",
    "precision_score(y_test_encoded, ada_pred, average='weighted', zero_division=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End Eric's AdaBoost modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funda's Linear Regression Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the \"roi\" column from X_train and X_test\n",
    "X_train_roi = X_train[[\"roi\"]]\n",
    "X_test_roi = X_test[[\"roi\"]]\n",
    "\n",
    "# Initialize and fit the model with Y as the independent variable and ROI as the dependent variable\n",
    "model = LinearRegression()\n",
    "model.fit(y_train_encoded.reshape(-1, 1), X_train_roi)\n",
    "\n",
    "# Predict ROI on the test set using Y\n",
    "predicted_roi = model.predict(y_test_encoded.reshape(-1, 1))\n",
    "\n",
    "# Calculate and print the metrics\n",
    "print(\"Mean Squared Error:\", mean_squared_error(X_test_roi, predicted_roi))\n",
    "print(\"R2 Score:\", r2_score(X_test_roi, predicted_roi))\n",
    "\n",
    "# Plotting the results with flipped axes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_encoded, X_test_roi, color='blue', label='Actual Data')\n",
    "plt.plot(y_test_encoded, predicted_roi, color='red', linewidth=2, label='Regression Line')\n",
    "plt.ylabel('ROI')\n",
    "plt.xlabel('Time (Y)')\n",
    "plt.title('Linear Regression of ROI Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End Funda's Linear Regression Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peta's Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vadim's Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a Random Forest Classifier model\n",
    "random_forest_model = RandomForestClassifier(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and save the Random Forest Classifier model using the training data\n",
    "random_forest_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Generate predictions from the model using the test data\n",
    "RFM_pred = random_forest_model.predict(X_test)\n",
    "\n",
    "# Review the predictions\n",
    "RFM_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying model scores\n",
    "print(f'Training score: {random_forest_model.score(X_train, y_train_encoded)}')\n",
    "print(f'Testing score: {random_forest_model.score(X_test, y_test_encoded)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the accuracy score for the test dataset\n",
    "accuracy_score(y_test_encoded, RFM_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the precision score for the test dataset.\n",
    "precision_score(y_test_encoded, RFM_pred, average='weighted', zero_division=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citations and Licenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citaions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Unemployment.csv**\n",
    "\n",
    "Economic Policy Institute, *State of Working America Data Library*, “Unemployment”, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Licenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TMBD_all_movies.csv**\n",
    "\n",
    "Copyright 2024 __[Alan Vourc'h](https://www.kaggle.com/alanvourch)__\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "You may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "> __http://www.apache.org/licenses/LICENSE-2.0__\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CCI_OECD.csv** and **US_inflation_rates.csv**\n",
    "\n",
    "CCO: Public Domain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_dev",
   "language": "python",
   "name": "ai_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
