{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Data explanation placeholder\n",
    "\n",
    "(Talk about original two notebooks?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in data\n",
    "df_movies = pd.read_csv(\"./Resources/movies_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Economics Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While classifying economic states is a complex and nuanced issue, it is not unreasonable to draw more broad-strokes generalizations about a given timeframe based on more limited factors. To serve the purposes of our modelling, the following three factors were chosen to highlight the economic status at a given movie's release date;\n",
    "\n",
    "* Consumer Confidence Indicator (CCI)\n",
    "* Consumer Price Index (CPI)\n",
    "* Unemployment Rate\n",
    "\n",
    "These features are further discussed below, and stand as adequate datapoints to answer three respective questions;\n",
    "\n",
    "* How likely are people to be spending money?\n",
    "* How much do things cost when they do spend money?\n",
    "* How many people have jobs to earn money to spend?\n",
    "\n",
    "As detailed below, this information came as monthly measures over several decades. To create our `Economic Climate` indicator - a classification as to whether or not the economics of a given time were on the better side for consumers - we will need to calculate a rolling 12-month percent change in the mean of those monthly values in order to show if a given feature was on a positive or negative trend for the provided period.\n",
    "\n",
    "---\n",
    "\n",
    "The following datasets are courtesy of __[Kaggle](https://www.kaggle.com/)__.\n",
    "\n",
    "### __['...CCI_OECD.csv'](https://www.kaggle.com/datasets/iqbalsyahakbar/cci-oecd)__\n",
    "\n",
    "*renamed from `DP_LIVE_16112023095843236.csv`*\n",
    "\n",
    "Per the Organisation for Economic Co-operation and Development (OECD);\n",
    "\n",
    "* The CCI is an indication of developments for future households' consumption and saving based on expected financial situation, sentiment regarding the general economic situation, employment status, and capacity for savings\n",
    "* An indicator above `100` indicates an optimistic outlook and a greater likliehood to spend money over cautious saving\n",
    "* An indicator below `100` indicates a pessimistic outlook and both a higher likeliehood to save money and a lower tendency to consume\n",
    "\n",
    "### __['...US_inflation_rates.csv'](https://www.kaggle.com/datasets/pavankrishnanarne/us-inflation-dataset-1947-present)__\n",
    "\n",
    "Per the dataset description;\n",
    "\n",
    "* The CPI is a critical economic indicator for measuring the purchasing power of money over time, measuring the average change over time in the prices paid by urban consumers for goods and services\n",
    "* The CPI is the value at the end of the respective month\n",
    "\n",
    "---\n",
    "\n",
    "The following datasets are courtesy of the __[Economic Policy Instituteâ€™s (EPI) State of Working America Data Library](https://www.epi.org/data/)__.\n",
    "\n",
    "### __['...Unemployment.csv'](https://www.epi.org/data/#?subject=unemp)__\n",
    "\n",
    "Per EPI description;\n",
    "\n",
    "* Unemployment is the share of the labor force wihout a job\n",
    "* Monthly percentages calculated as a rolling 12-month average (mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in data\n",
    "df_unemp = pd.read_csv(\"./Resources/EPI Data Library - Unemployment.csv\")\n",
    "df_cci = pd.read_csv(\"./Resources/CCI_OECD.csv\")\n",
    "df_inflation = pd.read_csv(\"./Resources/US_inflation_rates.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions\n",
    "\n",
    "Since each dataset will need similar preprocessing, the following functions will be used to help streamline the flow and code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal functions\n",
    "\n",
    "Applicable to all datasets\n",
    "\n",
    "#### Copying datasets\n",
    "\n",
    "Creating a working copy of a given dataset to preserve the original DF with unneeded features dropped\n",
    "\n",
    "#### Renaming needed features\n",
    "\n",
    "Renaming selected features for a given dataset\n",
    "\n",
    "#### Rolling mean and mean percent change\n",
    "\n",
    "Calculating the rolling 12-month mean and the rolling 12-month percent change for a given feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to copy a dataset with only the needed features\n",
    "def copy_df(df, features_to_keep):\n",
    "    df_copy = df[features_to_keep].copy()\n",
    "    return df_copy\n",
    "\n",
    "# Defining a function to rename needed features\n",
    "def rename_features(df, feature1, feature1new, feature2, feature2new):\n",
    "    df.rename(columns={\n",
    "        feature1: feature1new,\n",
    "        feature2: feature2new\n",
    "    }, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Defining a function to calculate the rolling 12-month means and percent changes\n",
    "# for a given feature\n",
    "def rolling_calcs(df, feature, feature_mean, feature_pct_chng):\n",
    "    df[feature_mean] = df[feature].rolling(window=12).mean()\n",
    "    df[feature_pct_chng] = df[feature_mean].pct_change(periods=12) * 100\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Situational functions\n",
    "\n",
    "Applicable to select datasets\n",
    "\n",
    "#### Datetime indexing\n",
    "\n",
    "Converting the feature containing the raw datetime information into a suitable datetime index\n",
    "\n",
    "*Cannot be used on `Unemployment` dataset*\n",
    "\n",
    "#### Removing '%'\n",
    "\n",
    "Removing the `'%'` from a given feature and converting the remaining `object` dtype to `float`\n",
    "\n",
    "*Specigically for `Unemployment` dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to set a `Date` feature as a datetime index\n",
    "def datetime_index(df, datetime_feature):\n",
    "    df[datetime_feature] = pd.to_datetime(df[datetime_feature])\n",
    "    df.set_index(datetime_feature, inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "# Defining a function to remove '%' and convert data `float`\n",
    "def convert_percentage(feature):\n",
    "    return float(feature.strip('%'))\n",
    "\n",
    "# Defining a function to apply `convert_percentage`\n",
    "def apply_percentage(df, feature):\n",
    "    df[feature] = df[feature].apply(convert_percentage)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCI\n",
    "\n",
    "#### Preprocessing of the `CCI_OECD.csv` dataset\n",
    "\n",
    "This dataset came with internaitonal records and unneeded features, so only records for US CCI will be retained. Once those records have been selected, the resulting DF will need to be prepared for concatenation with the remainined economic datasets. To do this, the `TIME` feature will be converted to datetime and set as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing `df_cci`\n",
    "df_cci.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beginning of limited EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beginning EDA on `df_cci`\n",
    "df_cci.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuing EDA\n",
    "df_cci.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuing EDA\n",
    "df_cci.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuing EDA\n",
    "df_cci['LOCATION'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting only domestic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying domestic data from `df_cci` to `df_cci_us` and removing unneeded features\n",
    "df_cci_us = df_cci.loc[df_cci['LOCATION'] == 'USA'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying `df_cci_us` and dropping unneeded features\n",
    "df_cci_form = copy_df(df_cci_us, ['TIME', 'Value'])\n",
    "\n",
    "# Renamining retained features\n",
    "df_cci_form = rename_features(\n",
    "    df_cci_form, 'TIME', 'Date', 'Value', 'CCI Value'\n",
    ")\n",
    "\n",
    "# Converting `Date` to a datetime index\n",
    "df_cci_form = datetime_index(df_cci_form, 'Date')\n",
    "\n",
    "# Calculating rolling 12-month means and percent change in means\n",
    "df_cci_form = rolling_calcs(\n",
    "    df_cci_form, 'CCI Value', 'CCI Rolling Mean', 'CCI Rolling Percent Change'\n",
    ")\n",
    "\n",
    "# Confirming `df_cci_form` ready to concatenate\n",
    "display(df_cci_form.head())\n",
    "display(df_cci_form.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inflation\n",
    "\n",
    "#### Preprocessing of the `US_inflation_rates.csv` dataset\n",
    "\n",
    "Seeing as the dataset came with only the needed features, little will be needed to prepare the DF for concatenation with the other economic datasets. `date` will be converted to datetime and set as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing `df_inflation`\n",
    "df_inflation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beginning of limited EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beginning of EDA on `df_inflation`\n",
    "df_inflation.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuing EDA\n",
    "df_inflation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuing EDA\n",
    "df_inflation.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying `df_inflation` and dropping unneeded features\n",
    "df_inflation_form = copy_df(df_inflation, ['date', 'value'])\n",
    "\n",
    "# Renamining retained features\n",
    "df_inflation_form = rename_features(\n",
    "    df_inflation_form, 'date', 'Date', 'value', 'CPI Value'\n",
    ")\n",
    "\n",
    "# Converting `Date` to a datetime index\n",
    "df_inflation_form = datetime_index(df_inflation_form, 'Date')\n",
    "\n",
    "# Calculating rolling 12-month means and percent change in means\n",
    "df_inflation_form = rolling_calcs(\n",
    "    df_inflation_form,\n",
    "    'CPI Value',\n",
    "    'CPI Rolling Mean',\n",
    "    'CPI Rolling Percent Change'\n",
    ")\n",
    "\n",
    "# Confirming `df_inflation_form` ready to concatenate\n",
    "display(df_inflation_form.head())\n",
    "display(df_inflation_form.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unemployment\n",
    "\n",
    "#### Preprocessing of the `Unemployment.csv` dataset\n",
    "\n",
    "This dataset came with unneeded features that will need to be dropped, as well as the needed features will need to be converted to `float`. Additionally, the `Date` feature will need to be converted to datetime and set to the index in preparation for concatenation with the other economic datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing `df_unemp`\n",
    "df_unemp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beginning of limited EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beginning EDA on `df_unemp`\n",
    "df_unemp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuing EDA\n",
    "df_unemp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuing EDA\n",
    "df_unemp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying `df_inflation` and dropping unneeded features\n",
    "df_inflation_form = copy_df(df_inflation, ['date', 'value'])\n",
    "\n",
    "# Renamining retained features\n",
    "df_inflation_form = rename_features(\n",
    "    df_inflation_form, 'date', 'Date', 'value', 'CPI Value'\n",
    ")\n",
    "\n",
    "# Converting `Date` to a datetime index\n",
    "df_inflation_form = datetime_index(df_inflation_form, 'Date')\n",
    "\n",
    "# Calculating rolling 12-month means and percent change in means\n",
    "df_inflation_form = rolling_calcs(\n",
    "    df_inflation_form,\n",
    "    'CPI Value',\n",
    "    'CPI Rolling Mean',\n",
    "    'CPI Rolling Percent Change'\n",
    ")\n",
    "\n",
    "# Confirming `df_inflation_form` ready to concatenate\n",
    "display(df_inflation_form.head())\n",
    "display(df_inflation_form.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unemployment\n",
    "\n",
    "#### Preprocessing of the `Unemployment.csv` dataset\n",
    "\n",
    "This dataset came with unneeded features that will need to be dropped, as well as the needed features will need to be converted to `float`. Additionally, the `Date` feature will need to be converted to datetime and set to the index in preparation for concatenation with the other economic datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing `df_unemp`\n",
    "df_unemp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beginning of limited EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beginning EDA on `df_unemp`\n",
    "df_unemp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuing EDA\n",
    "df_unemp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuing EDA\n",
    "df_unemp.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying defined functions (first pass)\n",
    "\n",
    "*Given the nature of the* `Date` *feature in this dataset, the datetime indexing will need to be handled outside of the defined functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying `df_unemp` and dropping unneeded features\n",
    "df_unemp_form = copy_df(df_unemp, ['Date', 'All'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming needed feature\n",
    "\n",
    "*This dataset only needed one feature,* `All`*, to be renmaned, this the* `rename_features` *defined function is not applicable*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the reatined feature\n",
    "df_unemp_form.rename(columns={'All': 'Unemployment Rate (%)'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datetime indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `Date` feature will need to be engineered into a workable datetime feature\n",
    "\n",
    "# Creating a dictionary of Months\n",
    "month_map = {\n",
    "    'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
    "    'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
    "}\n",
    "\n",
    "# Mapping integer month values to `Date Month`\n",
    "df_unemp_form['Date Month'] = df_unemp_form['Date'].str.slice(0,3).map(month_map)\n",
    "\n",
    "# Slicing `Date Year`\n",
    "df_unemp_form['Date Year'] = df_unemp_form['Date'].str.slice(4,8)\n",
    "\n",
    "# Converting `Date` to datetime using `Date Month` and `Date Year`\n",
    "df_unemp_form['Date'] = pd.to_datetime({\n",
    "    'year': df_unemp_form['Date Year'],\n",
    "    'month': df_unemp_form['Date Month'],\n",
    "    'day': 1\n",
    "})\n",
    "\n",
    "# Dropping engineered features `Date Month` and `Date Year`\n",
    "df_unemp_form.drop(columns=['Date Month', 'Date Year'], inplace=True)\n",
    "\n",
    "# Setting `Date` as index\n",
    "df_unemp_form.set_index('Date', inplace=True)\n",
    "\n",
    "# Ensuring index is sorted with ascending dates\n",
    "df_unemp_form.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying defined functions (second pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying `apply_percentage` to `Unemployment Rate (%)`\n",
    "df_unemp_form = apply_percentage(df_unemp_form, 'Unemployment Rate (%)')\n",
    "\n",
    "# Calculating rolling 12-month means and percent change in means\n",
    "df_unemp_form = rolling_calcs(\n",
    "    df_unemp_form,\n",
    "    'Unemployment Rate (%)',\n",
    "    'Unemployment Rate (%) Rolling Mean',\n",
    "    'Unemployment Rate Rolling Percent Change',\n",
    ")\n",
    "\n",
    "# Confirming `df_unemp_form` ready to concatenate\n",
    "display(df_unemp_form.head())\n",
    "display(df_unemp_form.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Economics\n",
    "\n",
    "#### Preprocessing of the `df_economics` DF\n",
    "\n",
    "With all datasets set to a monthly datetime index, the relevent features of all can be combined into one DF, and any NaN records can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconfirming total records for datasets\n",
    "df_cci_form.shape, df_inflation_form.shape, df_unemp_form.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the economic datasets into `df_economics`\n",
    "df_economics = pd.concat(\n",
    "    [\n",
    "        df_cci_form,\n",
    "        df_inflation_form,\n",
    "        df_unemp_form\n",
    "    ], axis=1, join='outer'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling `NaN` rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming total records\n",
    "df_economics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking total `NaN` records\n",
    "df_economics.isna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping `NaN` records\n",
    "df_economics.dropna(inplace=True)\n",
    "\n",
    "# Confirming remaining records\n",
    "df_economics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming final economic DF\n",
    "display(df_economics.head())\n",
    "display(df_economics.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering\n",
    "\n",
    "#### Engineering the economic target value\n",
    "\n",
    "As stated, the goal is to create an indicator for `Economic Climate` based on broad-strokes observations of our datasets. Having calculated the rolling 12-month percent change for each feature - based off the rolling 12-month mean - we can look for a positive or negative change in values and flag the movement accordingly. From there, we can make the following simple statements;\n",
    "\n",
    "* For **CCI**, a positive change is \"good\", as it indicates an increase in the likelihood of consumers to spend money\n",
    "* For **CPI**, a negative change is \"good\", as it indicates a decrease in the costs for goods and services\n",
    "* For **Unemployment Rate**, a negative change is \"good\", as it indicates an incrase in the population active in the workforce\n",
    "\n",
    "Therefore, we can interpret movement contrary to those changes as \"bad\". With this simplified view of the features, we can draw a classification as follows;\n",
    "\n",
    "* If **at least two (2) features** have a \"good\" value, we can set `Economic Climate` to `Comfortable to Good`\n",
    "* If **at least two (2) features** have a \"bad\" value, we can set `Economic Climate` to `Lean to Bad`\n",
    "\n",
    "In this way, we can gague whether the ecnomic state at a given rlease date supports or disproves our hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continued EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuing EDA\n",
    "df_economics.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifying `Economic Climate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of features\n",
    "features_to_flag = [\n",
    "    'CCI Rolling Percent Change',\n",
    "    'CPI Rolling Percent Change',\n",
    "    'Unemployment Rate Rolling Percent Change'\n",
    "]\n",
    "\n",
    "# Looping through `features_to_flag` to assign `positive` and `negative` indicators\n",
    "for col in df_economics[features_to_flag].columns:\n",
    "    new_col = str(col) + ' Flag'\n",
    "    df_economics.loc[df_economics[col] > 0, new_col] = 'positive'\n",
    "    df_economics.loc[df_economics[col] <= 0, new_col] = 'negative'\n",
    "\n",
    "# Creating a list flagged features\n",
    "flag_cols = [\n",
    "    'CCI Rolling Percent Change Flag',\n",
    "    'CPI Rolling Percent Change Flag',\n",
    "    'Unemployment Rate Rolling Percent Change Flag'\n",
    "]\n",
    "\n",
    "# Confirming indicators applied\n",
    "df_economics[flag_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring `Economic Climate` with a `PLACEHOLDER` value\n",
    "df_economics['Economic Climate'] = 'PLACEHOLDER'\n",
    "\n",
    "# CCI = \"positve\"/good, CPI = \"positive\"/bad, Unemployment = \"positive\"/bad\n",
    "# Classifying as `Lean to Bad`\n",
    "df_economics.loc[\n",
    "    (df_economics[flag_cols[0]] == 'positive') &\n",
    "    (df_economics[flag_cols[1]] == 'positive') &\n",
    "    (df_economics[flag_cols[2]] == 'positive'), 'Economic Climate'\n",
    "] = 'Lean to Bad'\n",
    "\n",
    "# CCI = \"positve\"/good, CPI = \"positive\"/bad, Unemployment = \"positive\"/bad\n",
    "# Classifying as `Lean to Bad`\n",
    "df_economics.loc[\n",
    "    (df_economics[flag_cols[0]] == 'positive') &\n",
    "    (df_economics[flag_cols[1]] == 'positive') &\n",
    "    (df_economics[flag_cols[2]] == 'negative'), 'Economic Climate'\n",
    "] = 'Comfortable to Good'\n",
    "\n",
    "# CCI = \"positve\"/good, CPI = \"positive\"/bad, Unemployment = \"positive\"/bad\n",
    "# Classifying as `Lean to Bad`\n",
    "df_economics.loc[\n",
    "    (df_economics[flag_cols[0]] == 'positive') &\n",
    "    (df_economics[flag_cols[1]] == 'negative') &\n",
    "    (df_economics[flag_cols[2]] == 'positive'), 'Economic Climate'\n",
    "] = 'Comfortable to Good'\n",
    "\n",
    "# CCI = \"positve\"/good, CPI = \"positive\"/bad, Unemployment = \"positive\"/bad\n",
    "# Classifying as `Lean to Bad`\n",
    "df_economics.loc[\n",
    "    (df_economics[flag_cols[0]] == 'negative') &\n",
    "    (df_economics[flag_cols[1]] == 'positive') &\n",
    "    (df_economics[flag_cols[2]] == 'positive'), 'Economic Climate'\n",
    "] = 'Lean to Bad'\n",
    "\n",
    "# CCI = \"positve\"/good, CPI = \"positive\"/bad, Unemployment = \"positive\"/bad\n",
    "# Classifying as `Lean to Bad`\n",
    "df_economics.loc[\n",
    "    (df_economics[flag_cols[0]] == 'negative') &\n",
    "    (df_economics[flag_cols[1]] == 'negative') &\n",
    "    (df_economics[flag_cols[2]] == 'positive'), 'Economic Climate'\n",
    "] = 'Lean to Bad'\n",
    "\n",
    "# CCI = \"positve\"/good, CPI = \"positive\"/bad, Unemployment = \"positive\"/bad\n",
    "# Classifying as `Lean to Bad`\n",
    "df_economics.loc[\n",
    "    (df_economics[flag_cols[0]] == 'negative') &\n",
    "    (df_economics[flag_cols[1]] == 'positive') &\n",
    "    (df_economics[flag_cols[2]] == 'negative'), 'Economic Climate'\n",
    "] = 'Lean to Bad'\n",
    "\n",
    "# CCI = \"positve\"/good, CPI = \"positive\"/bad, Unemployment = \"positive\"/bad\n",
    "# Classifying as `Lean to Bad`\n",
    "df_economics.loc[\n",
    "    (df_economics[flag_cols[0]] == 'positive') &\n",
    "    (df_economics[flag_cols[1]] == 'negative') &\n",
    "    (df_economics[flag_cols[2]] == 'negative'), 'Economic Climate'\n",
    "] = 'Comfortable to Good'\n",
    "\n",
    "# CCI = \"positve\"/good, CPI = \"positive\"/bad, Unemployment = \"positive\"/bad\n",
    "# Classifying as `Lean to Bad`\n",
    "df_economics.loc[\n",
    "    (df_economics[flag_cols[0]] == 'negative') &\n",
    "    (df_economics[flag_cols[1]] == 'negative') &\n",
    "    (df_economics[flag_cols[2]] == 'negative'), 'Economic Climate'\n",
    "] = 'Comfortable to Good'\n",
    "\n",
    "# Confirming classifications applied\n",
    "df_economics['Economic Climate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the index to recreate `Date` for later concatenation\n",
    "df_economics.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a 'Date' for a datetime index\n",
    "df_movies['Date'] = pd.to_datetime({\n",
    "    'year': df_movies['released_year'],\n",
    "    'month': df_movies['released_month'],\n",
    "    'day': df_movies['released_day']\n",
    "})\n",
    "\n",
    "# Setting `Date` as index\n",
    "df_movies.set_index('Date', inplace=True)\n",
    "\n",
    "# Ensuring index is sorted with ascending dates\n",
    "df_movies.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a `Year` and `Month` for concatenation\n",
    "df_economics['Year'] = df_economics['Date'].dt.strftime('%Y').astype(int)\n",
    "df_economics['Month'] = df_economics['Date'].dt.strftime('%m').astype(int)\n",
    "\n",
    "# Renaming to `Year` and `Month` for concatenation\n",
    "df_movies.rename(columns={\n",
    "'released_year': 'Year',\n",
    "'released_month': 'Month'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ecomonic records: 507\n",
      "Total movie records: 15363\n"
     ]
    }
   ],
   "source": [
    "# Confirming total records before concatenation\n",
    "print(f'Total ecomonic records: {df_economics.shape[0]}')\n",
    "print(f'Total movie records: {df_movies.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 12188\n"
     ]
    }
   ],
   "source": [
    "# Combining datasets through concatenation\n",
    "df_combined = pd.merge(df_economics, df_movies, how='left', on=['Year', 'Month'])\n",
    "\n",
    "# Confirming total records after concatenation\n",
    "print(f'Total records: {df_combined.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the eventual `Target` for modelling\n",
    "df_combined['Target'] = df_combined['critical_success'] + ' ' +\\\n",
    "                        df_combined['financial_success'] + ' ' +\\\n",
    "                        df_combined['Economic Climate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Target\n",
       "panned failure Lean to Bad                                    2200\n",
       "well liked failure Lean to Bad                                1897\n",
       "well liked failure Comfortable to Good                        1316\n",
       "panned failure Comfortable to Good                            1195\n",
       "alright failure Lean to Bad                                   1050\n",
       "alright failure Comfortable to Good                            753\n",
       "well liked excellent returns Lean to Bad                       709\n",
       "well liked excellent returns Comfortable to Good               527\n",
       "critical success failure Lean to Bad                           477\n",
       "well liked extraordinary returns Lean to Bad                   292\n",
       "critical success failure Comfortable to Good                   251\n",
       "well liked modest returns Lean to Bad                          221\n",
       "well liked extraordinary returns Comfortable to Good           201\n",
       "well liked moderate returns Lean to Bad                        193\n",
       "well liked modest returns Comfortable to Good                  164\n",
       "well liked moderate returns Comfortable to Good                139\n",
       "critical success excellent returns Lean to Bad                  76\n",
       "critical success excellent returns Comfortable to Good          65\n",
       "critical success extraordinary returns Lean to Bad              56\n",
       "critical success extraordinary returns Comfortable to Good      52\n",
       "panned broke even Lean to Bad                                   35\n",
       "alright excellent returns Lean to Bad                           28\n",
       "critical success modest returns Comfortable to Good             26\n",
       "alright excellent returns Comfortable to Good                   22\n",
       "critical success broke even Lean to Bad                         17\n",
       "critical success moderate returns Lean to Bad                   17\n",
       "alright moderate returns Lean to Bad                            16\n",
       "critical success moderate returns Comfortable to Good           16\n",
       "critical success broke even Comfortable to Good                 14\n",
       "critical success modest returns Lean to Bad                     14\n",
       "panned extraordinary returns Lean to Bad                        12\n",
       "panned broke even Comfortable to Good                           12\n",
       "panned excellent returns Lean to Bad                            12\n",
       "well liked broke even Lean to Bad                               11\n",
       "alright extraordinary returns Lean to Bad                       11\n",
       "panned moderate returns Lean to Bad                             10\n",
       "alright modest returns Comfortable to Good                      10\n",
       "panned modest returns Lean to Bad                                9\n",
       "well liked broke even Comfortable to Good                        9\n",
       "alright modest returns Lean to Bad                               9\n",
       "alright extraordinary returns Comfortable to Good                9\n",
       "panned excellent returns Comfortable to Good                     8\n",
       "alright moderate returns Comfortable to Good                     7\n",
       "panned moderate returns Comfortable to Good                      7\n",
       "panned modest returns Comfortable to Good                        6\n",
       "alright broke even Lean to Bad                                   3\n",
       "alright broke even Comfortable to Good                           2\n",
       "panned extraordinary returns Comfortable to Good                 2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "df_combined['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
=======
>>>>>>> 410515b8016c5c9c503bda8d5cfcdbe2ce600896
    "# Creating a list of features to drop\n",
    "cols_to_drop = [\n",
    "    'Date',\n",
    "    'CCI Rolling Mean',\n",
    "    'CPI Rolling Mean',\n",
    "    'Unemployment Rate (%) Rolling Mean',\n",
    "    'Economic Climate',\n",
    "    'Year',\n",
    "    'Month',\n",
    "    'id',\n",
    "    'cast',\n",
    "    'original_language',\n",
    "    'director',\n",
    "    'writers',\n",
    "    'producers',\n",
    "    'popularity', \n",
    "    'critical_success',\n",
    "    'financial_success',\n",
    "    'release_date',\n",
    "    'released_day',\n",
    "    'production_countries',\n",
    "    'status',\n",
    "    'spoken_languages'\n",
    "]\n",
    "\n",
    "# Dropping unneeded features\n",
    "df_combined.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping `NaN` records\n",
    "df_combined.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10025, 20)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming total records after concatenation\n",
    "print(f'Total records: {df_combined.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining columns to scale and encode\n",
    "col_to_scale = [\n",
    "    'CCI Value', 'CCI Rolling Percent Change', 'CPI Value',\n",
    "    'CPI Rolling Percent Change', 'Unemployment Rate (%)', \n",
    "    'Unemployment Rate Rolling Percent Change','vote_average', 'vote_count',\n",
    "    'revenue','runtime','budget', 'roi'\n",
    "]\n",
    "\n",
    "col_to_encode = [\n",
    "    'CCI Rolling Percent Change Flag', 'CPI Rolling Percent Change Flag',\n",
    "    'Unemployment Rate Rolling Percent Change Flag', 'title', 'original_title',\n",
    "    'genres', 'production_companies'\n",
    "]\n",
    "\n",
    "# Setup X and y variables\n",
    "X = df_combined.drop(columns='Target')\n",
    "y = df_combined['Target']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling and Econding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance for `StandardScalar()`\n",
    "scaler = StandardScaler()\n",
<<<<<<< HEAD
    "X_train_scaled = scaler.fit(X_train[col_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.transform(X_train[col_to_scale])\n",
    "X_test_scaled = scaler.transform(X_test[col_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=scaler.get_feature_names_out())\n",
    "X_test_scaled = pd.DataFrame(X_train_scaled, columns=scaler.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.40 GiB for an array with shape (52626, 13781) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m encoder_y\u001b[38;5;241m.\u001b[39mfit(y_train\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Transform each column into numpy arrays\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m X_train_encoded \u001b[38;5;241m=\u001b[39m encoder_x\u001b[38;5;241m.\u001b[39mtransform(X_train[col_to_encode]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      9\u001b[0m X_test_encoded \u001b[38;5;241m=\u001b[39m encoder_x\u001b[38;5;241m.\u001b[39mtransform(X_test[col_to_encode]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     11\u001b[0m y_train_encoded \u001b[38;5;241m=\u001b[39m encoder_y\u001b[38;5;241m.\u001b[39mtransform(y_train\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:958\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    952\u001b[0m out \u001b[38;5;241m=\u001b[39m sparse\u001b[38;5;241m.\u001b[39mcsr_matrix(\n\u001b[0;32m    953\u001b[0m     (data, indices, indptr),\n\u001b[0;32m    954\u001b[0m     shape\u001b[38;5;241m=\u001b[39m(n_samples, feature_indices[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]),\n\u001b[0;32m    955\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m    956\u001b[0m )\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse_output:\n\u001b[1;32m--> 958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    960\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1050\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1049\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1050\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_toarray_args(order, out)\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:1267\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 5.40 GiB for an array with shape (52626, 13781) and data type float64"
     ]
    }
   ],
   "source": [
    "encoder_x = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "encoder_x.fit(X_train[col_to_encode].values.reshape(-1,1))\n",
=======
>>>>>>> 410515b8016c5c9c503bda8d5cfcdbe2ce600896
    "\n",
    "# Fitting and transforming to `col_to_scale`\n",
    "X_train_scaled = scaler.fit_transform(X_train[col_to_scale])\n",
    "X_test_scaled = scaler.transform(X_test[col_to_scale])\n",
    "\n",
    "# Converting results to DF for later concatenation\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=col_to_scale)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=col_to_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance for `OneHotEncoder()` for `X_train[col_to_encode]`\n",
    "encoder_x = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fitting to `col_to_encode`\n",
    "encoder_x.fit(X_train[col_to_encode])\n",
    "\n",
    "# Creating an instance for `OneHotEncoder()` for `y_train`\n",
    "encoder_y = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "#Fitting\n",
    "encoder_y.fit(y_train.values.reshape(-1,1))\n",
    "\n",
    "# Transforming `X_train[col_to_encode]` and `X_test[col_to_encode]`\n",
    "X_train_encoded = encoder_x.transform(X_train[col_to_encode])\n",
    "X_test_encoded = encoder_x.transform(X_test[col_to_encode])\n",
    "\n",
    "# Transforming `y_train` and `y_test`\n",
    "y_train_encoded = encoder_y.transform(y_train.values.reshape(-1,1))\n",
    "y_test_encoded = encoder_y.transform(y_test.values.reshape(-1,1))\n",
    "\n",
    "# Converting results to DF for later concatenation\n",
    "X_train_encoded = pd.DataFrame(X_train_encoded, columns=encoder_x.get_feature_names_out())\n",
    "X_test_encoded = pd.DataFrame(X_test_encoded, columns=encoder_x.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the `col_to_scale` with `col_to_encode` for `X_train` and `X_test`\n",
    "X_train = pd.concat([X_train_scaled, X_train_encoded], axis=1)\n",
    "X_test = pd.concat([X_test_scaled, X_test_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming total records after concatenation\n",
    "print(f'Total X records: {X_train.shape[0] + X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Playtime!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eric's Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funda's Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalvin's Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Odele's Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peta's Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a logistic regression model.\n",
    "logistic_regression_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and save the logistic regression model using the training data\n",
    "df_combined_lr_model = logistic_regression_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "print(f\"Training Data Score: {df_combined_lr_model.score(X_train, y_train)}\")\n",
    "print(f\"Testing Data Score: {df_combined_lr_model.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from the logistic regression model using the test data\n",
    "lr_predictions = logistic_regression_model.predict(X_test)\n",
    "\n",
    "# Review the predictions\n",
    "testing_predections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the accuracy score for the test dataset.\n",
    "accuracy_score(y_test, testing_predections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the precision score for the test dataset.\n",
    "precision_score(y_test, testing_predections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vadim's Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
